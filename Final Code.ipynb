{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add01e5f",
   "metadata": {},
   "source": [
    "## INM363 Individual Project - Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aaa85e",
   "metadata": {},
   "source": [
    "#### Sahan Chowdhury"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941f8b5",
   "metadata": {},
   "source": [
    "#### This project looks at mobiliy patterns over time throughout COVID-19 and the impact this has had on the labour market specifically the gender pay gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import zscore\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import calendar\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd070d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data obtained from Google covid mobility reports\n",
    "#Dataset link: https://www.google.com/covid19/mobility/\n",
    "#Reference[1]: (Google, 2020)\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Global_Mobility_Report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd416f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining number of countries within the dataset\n",
    "df[\"country_region\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a concise summary of the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5752c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics for numerical features.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data descriptive transposed for better readability\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55879da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original data to make changes\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the unessesary columns of the dataset\n",
    "df_copy.drop(['sub_region_1', 'sub_region_2', 'metro_area', 'iso_3166_2_code', 'census_fips_code', 'place_id'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5914855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df_copy['date'] = pd.to_datetime(df_copy['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the mobility dataset\n",
    "df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee33ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising Missing values by country \n",
    "\n",
    "# Calculating the number of missing values for each country\n",
    "#Reference[2]: (alenavorushilova, 2020)\n",
    "missing_values_by_country = df_copy.groupby('country_region').apply(lambda x: x.isnull().sum())\n",
    "\n",
    "#Summing missing values to each country\n",
    "missing_values_by_country['total_missing'] = missing_values_by_country.sum(axis=1)\n",
    "\n",
    "# Sort the countries by the total number of missing values in descending order\n",
    "missing_values_by_country_sorted = missing_values_by_country.sort_values(by='total_missing', ascending=False)\n",
    "\n",
    "# Calculate the total number of missing values across all countries\n",
    "total_missing_all_countries = missing_values_by_country_sorted['total_missing'].sum()\n",
    "\n",
    "# Calculate the percentage contribution of missing values for each country\n",
    "missing_percentage_all = (missing_values_by_country_sorted['total_missing'] / total_missing_all_countries) * 100\n",
    "\n",
    "# Separate countries above and below the threshold of 2%\n",
    "above_threshold = missing_percentage_all[missing_percentage_all >= 2]\n",
    "below_threshold = missing_percentage_all[missing_percentage_all < 2]\n",
    "\n",
    "# Combine all below-threshold countries into one category called the rest\n",
    "below_threshold_sum = below_threshold.sum()\n",
    "above_threshold['The rest'] = below_threshold_sum\n",
    "\n",
    "# Creating and displaying the pie chart\n",
    "plt.pie(above_threshold, labels=above_threshold.index, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 9})  # Adjust font size\n",
    "plt.title('Percentage Contribution of Missing Data by Country')\n",
    "# Display the pie chart\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ef4d2",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e6b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d277c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each mobility category to visualise distribution \n",
    "ax = df_copy[['retail_and_recreation_percent_change_from_baseline', \n",
    "              'grocery_and_pharmacy_percent_change_from_baseline', \n",
    "              'parks_percent_change_from_baseline', \n",
    "              'transit_stations_percent_change_from_baseline', \n",
    "              'workplaces_percent_change_from_baseline', \n",
    "              'residential_percent_change_from_baseline']].hist(bins=30, figsize=(12, 10))\n",
    "\n",
    "plt.suptitle('Distribution of Mobility Changes Across Categories')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Correlation matrix\n",
    "correlation_matrix = df_copy[['retail_and_recreation_percent_change_from_baseline', \n",
    "                              'grocery_and_pharmacy_percent_change_from_baseline', \n",
    "                              'parks_percent_change_from_baseline', \n",
    "                              'transit_stations_percent_change_from_baseline', \n",
    "                              'workplaces_percent_change_from_baseline', \n",
    "                              'residential_percent_change_from_baseline']].corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd182c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising correlation matrix with a heatmap\n",
    "plt.figure()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix for Mobility Changes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6475d3",
   "metadata": {},
   "source": [
    "### Mobility pattern analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line graph plotting of mobility variables by weekly average \n",
    "#Reference[3]: (JPV, 2017)\n",
    "\n",
    "# Set 'date' as the index to create daily_mobility \n",
    "daily_mobility = df_copy.set_index('date')\n",
    "\n",
    "\n",
    "\n",
    "# Convert all columns to numeric \n",
    "daily_mobility_numeric = daily_mobility.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "#resampling the data to weekly frequency since weekly averages\n",
    "weekly_mobility = daily_mobility_numeric.resample('W').mean()\n",
    "\n",
    "# Plot time-series of mobility trends for each category (weekly data)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot each mobility category using the resampled weekly data\n",
    "plt.plot(weekly_mobility.index, weekly_mobility['retail_and_recreation_percent_change_from_baseline'], label='Retail & Recreation', color='green')\n",
    "plt.plot(weekly_mobility.index, weekly_mobility['workplaces_percent_change_from_baseline'], label='Workplaces', color='red')\n",
    "plt.plot(weekly_mobility.index, weekly_mobility['residential_percent_change_from_baseline'], label='Residential', color='blue')\n",
    "plt.plot(weekly_mobility.index, weekly_mobility['parks_percent_change_from_baseline'], label='Parks', color='purple')\n",
    "plt.plot(weekly_mobility.index, weekly_mobility['grocery_and_pharmacy_percent_change_from_baseline'], label='Grocery & Pharmacy', color='orange')\n",
    "plt.plot(weekly_mobility.index, weekly_mobility['transit_stations_percent_change_from_baseline'], label='Transit Stations', color='pink')\n",
    "\n",
    "#Baseline dotted line at y=0 this indicates baseline level\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Add shaded areas for key global events during the pandemic these are:\n",
    "plt.axvspan(pd.to_datetime('2020-03-01'), pd.to_datetime('2020-05-31'), color='gray', alpha=0.3, label='First Lockdown (Mar-May 2020)')\n",
    "plt.axvspan(pd.to_datetime('2020-12-01'), pd.to_datetime('2021-02-28'), color='lightblue', alpha=0.3, label='Second Wave & Lockdown (Dec 2020 - Feb 2021)')\n",
    "plt.axvspan(pd.to_datetime('2021-06-01'), pd.to_datetime('2021-08-31'), color='lightgreen', alpha=0.3, label='Delta Variant Peak (Jun-Aug 2021)')\n",
    "plt.axvspan(pd.to_datetime('2021-12-01'), pd.to_datetime('2022-02-28'), color='pink', alpha=0.3, label='Omicron Surge (Dec 2021 - Feb 2022)')\n",
    "\n",
    "#Vertical line for the WHO pandemic declaration on March 11, 2020\n",
    "plt.axvline(pd.to_datetime('2020-03-11'), color='black', linestyle='-.', linewidth=1.5, label='WHO Declared Pandemic (Mar 11, 2020)')\n",
    "\n",
    "#X axis customised using the reference\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator()) \n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "\n",
    "#X-axis labels rotated for easier readability \n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "#Display the plot \n",
    "plt.title('Time-Series of Weekly Mobility Changes During COVID-19', fontsize=14)\n",
    "plt.xlabel('Date (Months)', fontsize=12)\n",
    "plt.ylabel('Percent Change from Baseline', fontsize=12)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53033b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avg mobility variable percentage chnage for 5 key events during COVID heatmap \n",
    "\n",
    "#Date ranges for each heatmap\n",
    "date_ranges = {\n",
    "    'First Lockdown (Mar-May 2020)': ('2020-03-01', '2020-05-31'),\n",
    "    'Second Wave & Lockdown (Dec 2020 - Feb 2021)': ('2020-12-01', '2021-02-28'),\n",
    "    'Delta Variant Peak (Jun-Aug 2021)': ('2021-06-01', '2021-08-31'),\n",
    "    'Omicron Surge (Dec 2021 - Feb 2022)': ('2021-12-01', '2022-02-28'),\n",
    "    'Overall Trend (Mar 2020 - Feb 2022)': ('2020-03-01', '2022-02-28')\n",
    "}\n",
    "\n",
    "# Creating a figure with subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))  \n",
    "\n",
    "# For Loop to iterate through each date range to calculate averages and plot heatmaps\n",
    "for (event, (start_date, end_date)), ax in zip(date_ranges.items(), axes.flatten()):\n",
    "    filtered_data = df_copy[(df_copy['date'] >= start_date) & (df_copy['date'] <= end_date)]\n",
    "    \n",
    "    # Specifying the mobility columns \n",
    "    mobility_columns = ['retail_and_recreation_percent_change_from_baseline', 'workplaces_percent_change_from_baseline', 'residential_percent_change_from_baseline', 'parks_percent_change_from_baseline', 'grocery_and_pharmacy_percent_change_from_baseline', 'transit_stations_percent_change_from_baseline']\n",
    "    \n",
    "    #average mobility changes for the selected date range\n",
    "    average_mobility = filtered_data[mobility_columns].mean()\n",
    "    \n",
    "    #heatmap for the average mobility changes\n",
    "    sns.heatmap(average_mobility.values.reshape(1, -1), \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                yticklabels=[event], \n",
    "                xticklabels=mobility_columns, \n",
    "                ax=ax)\n",
    "    \n",
    "    ax.set_title(f'Average Mobility Changes: {event}')\n",
    "    ax.set_xlabel('Mobility Category')\n",
    "    ax.set_ylabel('Event')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ce766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF scores for mobility columns\n",
    "#Reference: (Tavares, 2017)\n",
    "\n",
    "#Creating a copy of the mobility dataframe, but only including the mobility data\n",
    "vif_data = df_copy[mobility_columns].copy().fillna(0) \n",
    "\n",
    "# Calculate VIF for each mobility variable\n",
    "vif_scores = pd.DataFrame()\n",
    "vif_scores[\"Feature\"] = vif_data.columns\n",
    "vif_scores[\"VIF\"] = [variance_inflation_factor(vif_data.values, i) for i in range(vif_data.shape[1])]\n",
    "\n",
    "# Display VIF scores\n",
    "vif_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127211e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Bar plot for key events for retail and recreation\n",
    "\n",
    "#Key events and dates\n",
    "date_ranges = {\n",
    "    'First Lockdown (Mar-May 2020)': ('2020-03-01', '2020-05-31'),\n",
    "    'Second Wave & Lockdown (Dec 2020 - Feb 2021)': ('2020-12-01', '2021-02-28'),\n",
    "    'Delta Variant Peak (Jun-Aug 2021)': ('2021-06-01', '2021-08-31'),\n",
    "    'Omicron Surge (Dec 2021 - Feb 2022)': ('2021-12-01', '2022-02-28'),}\n",
    "\n",
    "plt.figure(figsize=(15, 20)) \n",
    "\n",
    "#For Loop through each key event to calculate averages \n",
    "for idx, (event, (start_date, end_date)) in enumerate(date_ranges.items(), start=1):\n",
    "    filtered_data = df_copy[(df_copy['date'] >= start_date) & (df_copy['date'] <= end_date)]\n",
    "    \n",
    "    # Calculate average Retail & Recreation mobility for each country\n",
    "    average_mobility = filtered_data.groupby('country_region')['retail_and_recreation_percent_change_from_baseline'].mean().sort_values(ascending=False)\n",
    "\n",
    "    # Create a subplot for each event in a single column\n",
    "    plt.subplot(4, 1, idx)  \n",
    "    average_mobility.plot(kind='bar') \n",
    "    plt.title(event)  \n",
    "    plt.ylabel('Average Percent Change from Baseline') \n",
    "    plt.xticks(rotation=90)  \n",
    "\n",
    "plt.suptitle('Average Retail & Recreation Mobility Changes During Key Events', fontsize=16, y=1.02)  \n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edb59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for key events for workplaces mobility\n",
    "\n",
    "#Key event\n",
    "date_ranges = {\n",
    "    'First Lockdown (Mar-May 2020)': ('2020-03-01', '2020-05-31'),\n",
    "    'Second Wave & Lockdown (Dec 2020 - Feb 2021)': ('2020-12-01', '2021-02-28'),\n",
    "    'Delta Variant Peak (Jun-Aug 2021)': ('2021-06-01', '2021-08-31'),\n",
    "    'Omicron Surge (Dec 2021 - Feb 2022)': ('2021-12-01', '2022-02-28'),}\n",
    "\n",
    "plt.figure(figsize=(15, 20))  \n",
    "for idx, (event, (start_date, end_date)) in enumerate(date_ranges.items(), start=1):\n",
    "    filtered_data = df_copy[(df_copy['date'] >= start_date) & (df_copy['date'] <= end_date)]\n",
    "    average_mobility = filtered_data.groupby('country_region')['workplaces_percent_change_from_baseline'].mean().sort_values(ascending=False)\n",
    "\n",
    "    plt.subplot(4, 1, idx)  \n",
    "    average_mobility.plot(kind='bar') \n",
    "    plt.title(event)  \n",
    "    plt.ylabel('Average Percent Change from Baseline') \n",
    "    plt.xticks(rotation=90)  \n",
    "\n",
    "plt.suptitle('Average Workplaces Mobility Changes During Key Events', fontsize=16, y=1.02)  \n",
    "plt.tight_layout()  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for key events for residential mobility\n",
    "\n",
    "#Key events\n",
    "date_ranges = {\n",
    "    'First Lockdown (Mar-May 2020)': ('2020-03-01', '2020-05-31'),\n",
    "    'Second Wave & Lockdown (Dec 2020 - Feb 2021)': ('2020-12-01', '2021-02-28'),\n",
    "    'Delta Variant Peak (Jun-Aug 2021)': ('2021-06-01', '2021-08-31'),\n",
    "    'Omicron Surge (Dec 2021 - Feb 2022)': ('2021-12-01', '2022-02-28'),}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 20)) \n",
    "\n",
    "#For  Loop through each key event to calculate averages \n",
    "for idx, (event, (start_date, end_date)) in enumerate(date_ranges.items(), start=1):\n",
    "    filtered_data = df_copy[(df_copy['date'] >= start_date) & (df_copy['date'] <= end_date)]\n",
    "    average_mobility = filtered_data.groupby('country_region')['residential_percent_change_from_baseline'].mean().sort_values(ascending=False)\n",
    "\n",
    "    plt.subplot(4, 1, idx) \n",
    "    average_mobility.plot(kind='bar') \n",
    "    plt.title(event)  \n",
    "    plt.ylabel('Average Percent Change from Baseline') \n",
    "    plt.xticks(rotation=90)  \n",
    "\n",
    "plt.suptitle('Average Residential Mobility Changes During Key Events', fontsize=16, y=1.02)  \n",
    "plt.tight_layout()  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f38d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for key events for parks mobility\n",
    "\n",
    "#Key periods\n",
    "date_ranges = {\n",
    "    'First Lockdown (Mar-May 2020)': ('2020-03-01', '2020-05-31'),\n",
    "    'Second Wave & Lockdown (Dec 2020 - Feb 2021)': ('2020-12-01', '2021-02-28'),\n",
    "    'Delta Variant Peak (Jun-Aug 2021)': ('2021-06-01', '2021-08-31'),\n",
    "    'Omicron Surge (Dec 2021 - Feb 2022)': ('2021-12-01', '2022-02-28'),}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 20))\n",
    "\n",
    "#For Loop through each key event to calculate averages\n",
    "for idx, (event, (start_date, end_date)) in enumerate(date_ranges.items(), start=1):\n",
    "    filtered_data = df_copy[(df_copy['date'] >= start_date) & (df_copy['date'] <= end_date)]\n",
    "    \n",
    "    # Calculate average Parks mobility for each country\n",
    "    average_mobility = filtered_data.groupby('country_region')['parks_percent_change_from_baseline'].mean().sort_values(ascending=False)\n",
    "\n",
    "    plt.subplot(4, 1, idx)  \n",
    "    average_mobility.plot(kind='bar')\n",
    "    plt.title(event)\n",
    "    plt.ylabel('Average Percent Change from Baseline')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "plt.suptitle('Average Parks Mobility Changes During Key Events', fontsize=16, y=1.02)\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for key events for grocery and pharmacy mobility\n",
    "\n",
    "#Key time period chosen for analysis\n",
    "date_ranges = {\n",
    "    'First Lockdown (Mar-May 2020)': ('2020-03-01', '2020-05-31'),\n",
    "    'Second Wave & Lockdown (Dec 2020 - Feb 2021)': ('2020-12-01', '2021-02-28'),\n",
    "    'Delta Variant Peak (Jun-Aug 2021)': ('2021-06-01', '2021-08-31'),\n",
    "    'Omicron Surge (Dec 2021 - Feb 2022)': ('2021-12-01', '2022-02-28'),}\n",
    "\n",
    "# Create a figure for the bar plots with larger size\n",
    "plt.figure(figsize=(15, 20))  # Adjust size as needed\n",
    "\n",
    "#For Loop through each key event to calculate averages \n",
    "for idx, (event, (start_date, end_date)) in enumerate(date_ranges.items(), start=1):\n",
    "    \n",
    "    filtered_data = df_copy[(df_copy['date'] >= start_date) & (df_copy['date'] <= end_date)]\n",
    "    \n",
    "    # Calculate average Grocery & Pharmacy mobility for each country\n",
    "    average_mobility = filtered_data.groupby('country_region')['grocery_and_pharmacy_percent_change_from_baseline'].mean().sort_values(ascending=False)\n",
    "\n",
    "   \n",
    "    plt.subplot(4, 1, idx)  \n",
    "    average_mobility.plot(kind='bar')\n",
    "    plt.title(event)\n",
    "    plt.ylabel('Average Percent Change from Baseline')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "plt.suptitle('Average Grocery & Pharmacy Mobility Changes During Key Events', fontsize=16, y=1.02)\n",
    "plt.tight_layout()  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for key events for transit stations mobility\n",
    "\n",
    "#Key time periods\n",
    "date_ranges = {\n",
    "    'First Lockdown (Mar-May 2020)': ('2020-03-01', '2020-05-31'),\n",
    "    'Second Wave & Lockdown (Dec 2020 - Feb 2021)': ('2020-12-01', '2021-02-28'),\n",
    "    'Delta Variant Peak (Jun-Aug 2021)': ('2021-06-01', '2021-08-31'),\n",
    "    'Omicron Surge (Dec 2021 - Feb 2022)': ('2021-12-01', '2022-02-28'),}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 20))  \n",
    "\n",
    "# For Loop through each key event to calculate averages \n",
    "for idx, (event, (start_date, end_date)) in enumerate(date_ranges.items(), start=1):\n",
    "    filtered_data = df_copy[(df_copy['date'] >= start_date) & (df_copy['date'] <= end_date)]\n",
    "    \n",
    "    # Calculate average Transit Stations mobility for each country\n",
    "    average_mobility = filtered_data.groupby('country_region')['transit_stations_percent_change_from_baseline'].mean().sort_values(ascending=False)\n",
    "\n",
    "    # Create a subplot for each event in a single column\n",
    "    plt.subplot(4, 1, idx)  \n",
    "    average_mobility.plot(kind='bar')\n",
    "    plt.title(event)\n",
    "    plt.ylabel('Average Percent Change from Baseline')\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "plt.suptitle('Average Transit Stations Mobility Changes During Key Events', fontsize=16, y=1.02)\n",
    "plt.tight_layout()  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7e13b",
   "metadata": {},
   "source": [
    "### Chloropleth mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the world map shapefile\n",
    "# Reference: (Naturaldisasters.ai, 2023)\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique country names from the naturalearth_lowres shapefile\n",
    "unique_countries_shapefile = world['name'].unique()\n",
    "\n",
    "# Print the unique country names in the shapefile\n",
    "print(\"\\nUnique country names in the naturalearth_lowres shapefile:\")\n",
    "print(unique_countries_shapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff44ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Country name adjustment with shapefile\n",
    "\n",
    "# Get unique country names from df_copy\n",
    "unique_countries_dataset = df_copy['country_region'].unique()\n",
    "\n",
    "# Print the unique country names in df_copy\n",
    "print(\"Unique country names in your dataset:\")\n",
    "print(unique_countries_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403caa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Country name comparison between df_copy and shapefile\n",
    "\n",
    "# Converting both lists to sets\n",
    "set_dataset = set(unique_countries_dataset)\n",
    "set_shapefile = set(unique_countries_shapefile)\n",
    "\n",
    "# Find countries in the df_copy missing in the shapefile\n",
    "missing_in_shapefile = set_dataset - set_shapefile\n",
    "print(\"\\nCountry names in df_copy that are missing from the shapefile:\")\n",
    "print(missing_in_shapefile)\n",
    "print(f\"Count: {len(missing_in_shapefile)}\")\n",
    "\n",
    "# Find countries in the shapefile missing in df_copy\n",
    "missing_in_dataset = set_shapefile - set_dataset\n",
    "print(\"\\nCountry names in the shapefile that are missing from df_copy:\")\n",
    "print(missing_in_dataset)\n",
    "print(f\"Count: {len(missing_in_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa70ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of country names in your dataset to the names in the shapefile\n",
    "country_name_mapping = {\n",
    "    'Mauritius': None,  \n",
    "    'Cape Verde': None, \n",
    "    'Aruba': None,  \n",
    "    'The Bahamas': 'Bahamas',  \n",
    "    'Bahrain': None,  \n",
    "    'Hong Kong': None,  \n",
    "    'Malta': None,\n",
    "    'Liechtenstein': None,  \n",
    "    'Barbados': None,  \n",
    "    'Myanmar (Burma)': 'Myanmar',  \n",
    "    'Dominican Republic': 'Dominican Rep.', \n",
    "    'Bosnia and Herzegovina': 'Bosnia and Herz.',  \n",
    "    'Antigua and Barbuda': None,  \n",
    "    'Réunion': None,  \n",
    "    'Singapore': None,  \n",
    "    'United States': 'United States of America'  \n",
    "}\n",
    "\n",
    "# Apply the mapping to df_copy\n",
    "df_copy['country_region'] = df_copy['country_region'].replace(country_name_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d991e",
   "metadata": {},
   "source": [
    "### Retail and recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: (ryilkici, 2020)\n",
    "#Reference: (Plotly, 2020)\n",
    "#Interactive chloropleth map \n",
    "\n",
    "#New column for week is created \n",
    "df_copy['week'] = df_copy['date'].dt.to_period('W').dt.to_timestamp()\n",
    "\n",
    "#Group by country and week to calculate the weekly average for 'retail_and_recreation_percent_change_from_baseline'\n",
    "weekly_avg_df = df_copy.groupby(['country_region', 'week'])['retail_and_recreation_percent_change_from_baseline'].mean().reset_index()\n",
    "\n",
    "\n",
    "#Weekly data is erged with the world map based on country name\n",
    "merged_data = pd.merge(world[['name', 'geometry']], weekly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "#Interactive map using plotly\n",
    "fig = px.choropleth(\n",
    "    merged_data,\n",
    "    geojson=world.__geo_interface__,  \n",
    "    locations='name',  \n",
    "    locationmode='country names',\n",
    "    color='retail_and_recreation_percent_change_from_baseline',\n",
    "    animation_frame='week',  \n",
    "    color_continuous_scale='YlOrRd', \n",
    "    title='Weekly Mobility Changes in Retail & Recreation',\n",
    "    labels={'retail_and_recreation_percent_change_from_baseline': 'Change from Baseline (%)'},\n",
    "    hover_name='name',  \n",
    "    projection=\"natural earth\"\n",
    ")\n",
    "\n",
    "#\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "#Adjusting margins on map \n",
    "fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d0e67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choropleth map of key events\n",
    "\n",
    "# Making a month column\n",
    "df_copy['month'] = df_copy['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Taking monthly averages for retail and recreation \n",
    "monthly_avg_df = df_copy.groupby(['country_region', 'month'])['retail_and_recreation_percent_change_from_baseline'].mean().reset_index()\n",
    "\n",
    "# Merge the weekly and monthly data with the world map\n",
    "merged_weekly_data = pd.merge(world[['name', 'geometry']], weekly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "merged_monthly_data = pd.merge(world[['name', 'geometry']], monthly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "# Define key events and their time frames, including the new timeframe for October 2022\n",
    "key_events = {\n",
    "    'WHO Declares Pandemic (March 11, 2020)': ('week', pd.Timestamp('2020-03-09')),\n",
    "    'First Lockdown (April 2020)': ('month', pd.Timestamp('2020-04-01')),\n",
    "    'Delta Variant Peak (August 2021)': ('month', pd.Timestamp('2021-08-01')),\n",
    "    'Omicron Surge (January 2022)': ('month', pd.Timestamp('2022-01-01')),\n",
    "    'End of Data (October 2022)': ('month', pd.Timestamp('2022-10-01'))\n",
    "}\n",
    "\n",
    "# Subplot\n",
    "fig, axes = plt.subplots(5, 1, figsize=(12, 25))  # Change to 5 rows, 1 column\n",
    "axes = axes.flatten()\n",
    "\n",
    "# For loop to loop through each time event\n",
    "for ax, (event, (freq, time_frame)) in zip(axes, key_events.items()):\n",
    "    snapshot = merged_weekly_data[merged_weekly_data['week'] == time_frame] if freq == 'week' else merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "    # Plotting the data\n",
    "    snapshot.plot(column='retail_and_recreation_percent_change_from_baseline', ax=ax, legend=True, cmap='coolwarm', missing_kwds={'color': 'white'}, legend_kwds={'label': event})\n",
    "    ax.set_title(event)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a41858",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#identfying the min and max for each event\n",
    "\n",
    "# Looping through each key event to identify the top and bottom changes in mobility\n",
    "for event, (freq, time_frame) in key_events.items():\n",
    "    print(f\"\\nMost and Least Changes for {event} ({time_frame}):\")\n",
    "    \n",
    "    snapshot = merged_weekly_data[merged_weekly_data['week'] == time_frame] if freq == 'week' else merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "\n",
    "    # Drop rows with NaN values in the change column\n",
    "    snapshot = snapshot.dropna(subset=['retail_and_recreation_percent_change_from_baseline'])\n",
    "    \n",
    "    # Sort values by 'retail_and_recreation_percent_change_from_baseline top 10 for most and least'\n",
    "    most_changed = snapshot.nlargest(10, 'retail_and_recreation_percent_change_from_baseline')\n",
    "    least_changed = snapshot.nsmallest(10, 'retail_and_recreation_percent_change_from_baseline')\n",
    "    #display\n",
    "    print(\"\\nCountries with the most positive changes:\")\n",
    "    print(most_changed[['name', 'retail_and_recreation_percent_change_from_baseline']])\n",
    "    \n",
    "    print(\"\\nCountries with the most negative changes:\")\n",
    "    print(least_changed[['name', 'retail_and_recreation_percent_change_from_baseline']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c825c",
   "metadata": {},
   "source": [
    "### Grocery and pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: (ryilkici, 2020)\n",
    "# Reference: (Plotly, 2020)\n",
    "# Interactive choropleth map for Grocery and Pharmacy mobility\n",
    "\n",
    "\n",
    "# Group by country and week to calculate the weekly average for 'grocery_and_pharmacy_percent_change_from_baseline'\n",
    "weekly_avg_df = df_copy.groupby(['country_region', 'week'])['grocery_and_pharmacy_percent_change_from_baseline'].mean().reset_index()\n",
    "\n",
    "# Weekly data is merged with the world map based on country name\n",
    "merged_data = pd.merge(world[['name', 'geometry']], weekly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "# Interactive map using Plotly\n",
    "fig = px.choropleth(\n",
    "    merged_data,\n",
    "    geojson=world.__geo_interface__,  \n",
    "    locations='name',  \n",
    "    locationmode='country names',\n",
    "    color='grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    animation_frame='week',  \n",
    "    color_continuous_scale='YlOrRd', \n",
    "    title='Weekly Mobility Changes in Grocery & Pharmacy',\n",
    "    labels={'grocery_and_pharmacy_percent_change_from_baseline': 'Change from Baseline (%)'},\n",
    "    hover_name='name',  \n",
    "    projection=\"natural earth\"\n",
    ")\n",
    "\n",
    "# Update geographical settings\n",
    "fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "# Adjusting margins on map \n",
    "fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n",
    "\n",
    "# Display the interactive map\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choropleth map of key events for Grocery & Pharmacy mobility\n",
    "\n",
    "# Making a month column\n",
    "df_copy['month'] = df_copy['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Taking monthly averages for grocery and pharmacy\n",
    "monthly_avg_df = df_copy.groupby(['country_region', 'month'])['grocery_and_pharmacy_percent_change_from_baseline'].mean().reset_index()\n",
    "\n",
    "# Merge the weekly and monthly data with the world map\n",
    "merged_weekly_data = pd.merge(world[['name', 'geometry']], weekly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "merged_monthly_data = pd.merge(world[['name', 'geometry']], monthly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "# Define key events and their time frames, including the new timeframe for October 2022\n",
    "key_events = {\n",
    "    'WHO Declares Pandemic (March 11, 2020)': ('week', pd.Timestamp('2020-03-09')),\n",
    "    'First Lockdown (April 2020)': ('month', pd.Timestamp('2020-04-01')),\n",
    "    'Delta Variant Peak (August 2021)': ('month', pd.Timestamp('2021-08-01')),\n",
    "    'Omicron Surge (January 2022)': ('month', pd.Timestamp('2022-01-01')),\n",
    "    'End of Data (October 2022)': ('month', pd.Timestamp('2022-10-01'))\n",
    "}\n",
    "\n",
    "# Subplot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  \n",
    "axes = axes.flatten()\n",
    "\n",
    "# For loop to loop through each time event\n",
    "for ax, (event, (freq, time_frame)) in zip(axes, key_events.items()):\n",
    "    snapshot = merged_weekly_data[merged_weekly_data['week'] == time_frame] if freq == 'week' else merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "    # Plotting the data\n",
    "    snapshot.plot(column='grocery_and_pharmacy_percent_change_from_baseline', ax=ax, legend=True, cmap='coolwarm', \n",
    "                  missing_kwds={'color': 'white'}, legend_kwds={'label': event})\n",
    "    ax.set_title(event)\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2e827",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Identifying the min and max for each event for Grocery & Pharmacy mobility\n",
    "\n",
    "# Looping through each key event to identify the top and bottom changes in mobility\n",
    "for event, (freq, time_frame) in key_events.items():\n",
    "    print(f\"\\nMost and Least Changes for {event} ({time_frame}):\")\n",
    "    \n",
    "    # Determine the relevant snapshot based on frequency\n",
    "    snapshot = merged_weekly_data[merged_weekly_data['week'] == time_frame] if freq == 'week' else merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "\n",
    "    # Drop rows with NaN values in the change column\n",
    "    snapshot = snapshot.dropna(subset=['grocery_and_pharmacy_percent_change_from_baseline'])\n",
    "\n",
    "    # Identify the top 10 and bottom 10 changes\n",
    "    most_changed = snapshot.nlargest(10, 'grocery_and_pharmacy_percent_change_from_baseline')\n",
    "    least_changed = snapshot.nsmallest(10, 'grocery_and_pharmacy_percent_change_from_baseline')\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nCountries with the most positive changes:\")\n",
    "    print(most_changed[['name', 'grocery_and_pharmacy_percent_change_from_baseline']])\n",
    "    \n",
    "    print(\"\\nCountries with the most negative changes:\")\n",
    "    print(least_changed[['name', 'grocery_and_pharmacy_percent_change_from_baseline']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b680cb6",
   "metadata": {},
   "source": [
    "### Parks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b4c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chloropleth map of key events for Parks\n",
    "\n",
    "\n",
    "# Take monthly averages for parks\n",
    "monthly_avg_df = df_copy.groupby(['country_region', 'month'])['parks_percent_change_from_baseline'].mean().reset_index()\n",
    "\n",
    "# Merge the monthly data with the world map\n",
    "merged_monthly_data = pd.merge(world[['name', 'geometry']], monthly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "# Define key events and their time frames\n",
    "key_events = {\n",
    "    'WHO Declares Pandemic (March 11, 2020)': ('month', pd.Timestamp('2020-03-01')),\n",
    "    'First Lockdown (April 2020)': ('month', pd.Timestamp('2020-04-01')),\n",
    "    'Delta Variant Peak (August 2021)': ('month', pd.Timestamp('2021-08-01')),\n",
    "    'Omicron Surge (January 2022)': ('month', pd.Timestamp('2022-01-01')),\n",
    "    'End of Data (October 2022)': ('month', pd.Timestamp('2022-10-01'))\n",
    "}\n",
    "\n",
    "# Subplot for Parks Mobility\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  # Adjust subplot layout as needed\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through each key event to plot parks data\n",
    "for ax, (event, (freq, time_frame)) in zip(axes, key_events.items()):\n",
    "    # Get the snapshot based on the frequency (in this case, it will always be monthly)\n",
    "    snapshot = merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "    \n",
    "    # Plotting the data for Parks\n",
    "    snapshot.plot(column='parks_percent_change_from_baseline', ax=ax, legend=True, cmap='coolwarm',\n",
    "                  missing_kwds={'color': 'white'}, legend_kwds={'label': event})\n",
    "    \n",
    "    ax.set_title(event)\n",
    "    ax.set_axis_off()  \n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the min and max for each event for Parks mobility\n",
    "\n",
    "# Looping through each key event to identify the top and bottom changes in mobility\n",
    "for event, (freq, time_frame) in key_events.items():\n",
    "    print(f\"\\nMost and Least Changes for {event} ({time_frame}):\")\n",
    "    \n",
    "    # Determine the relevant snapshot based on frequency\n",
    "    snapshot = merged_weekly_data[merged_weekly_data['week'] == time_frame] if freq == 'week' else merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "\n",
    "    # Drop rows with NaN values in the change column\n",
    "    snapshot = snapshot.dropna(subset=['parks_percent_change_from_baseline'])\n",
    "\n",
    "    # Identify the top 10 and bottom 10 changes\n",
    "    most_changed = snapshot.nlargest(10, 'parks_percent_change_from_baseline')\n",
    "    least_changed = snapshot.nsmallest(10, 'parks_percent_change_from_baseline')\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nCountries with the most positive changes:\")\n",
    "    print(most_changed[['name', 'parks_percent_change_from_baseline']])\n",
    "    \n",
    "    print(\"\\nCountries with the most negative changes:\")\n",
    "    print(least_changed[['name', 'parks_percent_change_from_baseline']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acafda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afa3c6",
   "metadata": {},
   "source": [
    "### Workplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c74a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chloropleth map of key events for Workplace\n",
    "\n",
    "# Making a month column\n",
    "df_copy['month'] = df_copy['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Taking monthly averages for Workplace\n",
    "monthly_avg_df = df_copy.groupby(['country_region', 'month'])['workplaces_percent_change_from_baseline'].mean().reset_index()\n",
    "\n",
    "# Merge the monthly data with the world map\n",
    "merged_monthly_data = pd.merge(world[['name', 'geometry']], monthly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "# Define key events and their time frames\n",
    "key_events = {\n",
    "    'WHO Declares Pandemic (March 11, 2020)': pd.Timestamp('2020-03-01'),\n",
    "    'First Lockdown (April 2020)': pd.Timestamp('2020-04-01'),\n",
    "    'Delta Variant Peak (August 2021)': pd.Timestamp('2021-08-01'),\n",
    "    'Omicron Surge (January 2022)': pd.Timestamp('2022-01-01'),\n",
    "    'End of Data (October 2022)': pd.Timestamp('2022-10-01')\n",
    "}\n",
    "\n",
    "# Subplot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  \n",
    "axes = axes.flatten()\n",
    "\n",
    "# For loop to loop through each time event\n",
    "for ax, (event, time_frame) in zip(axes, key_events.items()):\n",
    "    snapshot = merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "    \n",
    "    # Plotting the data for Workplace\n",
    "    snapshot.plot(column='workplaces_percent_change_from_baseline', ax=ax, legend=True, cmap='coolwarm', \n",
    "                  missing_kwds={'color': 'white'}, legend_kwds={'label': event})\n",
    "    ax.set_title(event)\n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b90cc1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Identifying the min and max for each event for Workplace mobility\n",
    "\n",
    "merged_monthly_data = pd.merge(world[['name', 'geometry']], monthly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "# Define key events and their time frames\n",
    "key_events = {\n",
    "    'WHO Declares Pandemic (March 11, 2020)': pd.Timestamp('2020-03-01'),\n",
    "    'First Lockdown (April 2020)': pd.Timestamp('2020-04-01'),\n",
    "    'Delta Variant Peak (August 2021)': pd.Timestamp('2021-08-01'),\n",
    "    'Omicron Surge (January 2022)': pd.Timestamp('2022-01-01'),\n",
    "    'End of Data (October 2022)': pd.Timestamp('2022-10-01')\n",
    "}\n",
    "\n",
    "# Looping through each key event to identify the top and bottom changes in mobility\n",
    "for event, time_frame in key_events.items():\n",
    "    print(f\"\\nMost and Least Changes for {event} ({time_frame}):\")\n",
    "    \n",
    "    # Determine the relevant snapshot based on frequency\n",
    "    snapshot = merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "\n",
    "    # Drop rows with NaN values in the change column\n",
    "    snapshot = snapshot.dropna(subset=['workplaces_percent_change_from_baseline'])\n",
    "\n",
    "    # Identify the top 10 and bottom 10 changes\n",
    "    most_changed = snapshot.nlargest(10, 'workplaces_percent_change_from_baseline')\n",
    "    least_changed = snapshot.nsmallest(10, 'workplaces_percent_change_from_baseline')\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nCountries with the most positive changes:\")\n",
    "    print(most_changed[['name', 'workplaces_percent_change_from_baseline']])\n",
    "    \n",
    "    print(\"\\nCountries with the most negative changes:\")\n",
    "    print(least_changed[['name', 'workplaces_percent_change_from_baseline']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dcdff8",
   "metadata": {},
   "source": [
    "### Transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chloropleth map of key events for Transit Stations\n",
    "\n",
    "# Making a month column\n",
    "df_copy['month'] = df_copy['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Taking monthly averages for Transit Stations\n",
    "monthly_avg_df = df_copy.groupby(['country_region', 'month'])['transit_stations_percent_change_from_baseline'].mean().reset_index()\n",
    "\n",
    "# Merge the monthly data with the world map\n",
    "merged_monthly_data = pd.merge(world[['name', 'geometry']], monthly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "# Define key events and their time frames\n",
    "key_events = {\n",
    "    'WHO Declares Pandemic (March 11, 2020)': pd.Timestamp('2020-03-01'),\n",
    "    'First Lockdown (April 2020)': pd.Timestamp('2020-04-01'),\n",
    "    'Delta Variant Peak (August 2021)': pd.Timestamp('2021-08-01'),\n",
    "    'Omicron Surge (January 2022)': pd.Timestamp('2022-01-01'),\n",
    "    'End of Data (October 2022)': pd.Timestamp('2022-10-01')\n",
    "}\n",
    "\n",
    "# Subplot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  \n",
    "axes = axes.flatten()\n",
    "\n",
    "# For loop to loop through each time event\n",
    "for ax, (event, time_frame) in zip(axes, key_events.items()):\n",
    "    snapshot = merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "    \n",
    "    # Plotting the data for Transit Stations\n",
    "    snapshot.plot(column='transit_stations_percent_change_from_baseline', ax=ax, legend=True, cmap='coolwarm', missing_kwds={'color': 'white'}, legend_kwds={'label': event})\n",
    "    ax.set_title(event)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44cdcf5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Transit\n",
    "#Define key events and their time frames\n",
    "key_events = {\n",
    "    'WHO Declares Pandemic (March 11, 2020)': ('month', pd.Timestamp('2020-03-01')),\n",
    "    'First Lockdown (April 2020)': ('month', pd.Timestamp('2020-04-01')),\n",
    "    'Delta Variant Peak (August 2021)': ('month', pd.Timestamp('2021-08-01')),\n",
    "    'Omicron Surge (January 2022)': ('month', pd.Timestamp('2022-01-01')),\n",
    "    'End of Data (October 2022)': ('month', pd.Timestamp('2022-10-01'))\n",
    "}\n",
    "\n",
    "#Loop through each key event to identify the top and bottom changes in mobility for Transit Stations\n",
    "for event, (freq, time_frame) in key_events.items():\n",
    "    print(f\"\\nMost and Least Changes for {event} ({time_frame}):\")\n",
    "    \n",
    "    snapshot = merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "\n",
    "    # Drop rows with NaN values in the change column\n",
    "    snapshot = snapshot.dropna(subset=['transit_stations_percent_change_from_baseline'])\n",
    "\n",
    "    # Identify the top 10 and bottom 10 changes\n",
    "    most_changed = snapshot.nlargest(10, 'transit_stations_percent_change_from_baseline')\n",
    "    least_changed = snapshot.nsmallest(10, 'transit_stations_percent_change_from_baseline')\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nCountries with the most positive changes:\")\n",
    "    print(most_changed[['name', 'transit_stations_percent_change_from_baseline']])\n",
    "    \n",
    "    print(\"\\nCountries with the most negative changes:\")\n",
    "    print(least_changed[['name', 'transit_stations_percent_change_from_baseline']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab181439",
   "metadata": {},
   "source": [
    "### Residential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62896ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chloropleth map of key events for Residential\n",
    "\n",
    "# Taking monthly averages for Residential\n",
    "monthly_avg_df = df_copy.groupby(['country_region', 'month'])['residential_percent_change_from_baseline'].mean().reset_index()\n",
    "\n",
    "# Merge the monthly data with the world map\n",
    "merged_monthly_data = pd.merge(world[['name', 'geometry']], monthly_avg_df, how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "# Define key events and their time frames, including the new timeframe for October 2022\n",
    "key_events = {\n",
    "    'WHO Declares Pandemic (March 11, 2020)': ('month', pd.Timestamp('2020-03-01')),\n",
    "    'First Lockdown (April 2020)': ('month', pd.Timestamp('2020-04-01')),\n",
    "    'Delta Variant Peak (August 2021)': ('month', pd.Timestamp('2021-08-01')),\n",
    "    'Omicron Surge (January 2022)': ('month', pd.Timestamp('2022-01-01')),\n",
    "    'End of Data (October 2022)': ('month', pd.Timestamp('2022-10-01'))\n",
    "}\n",
    "\n",
    "# Subplot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))  \n",
    "axes = axes.flatten()\n",
    "\n",
    "# For loop to loop through each time event\n",
    "for ax, (event, (freq, time_frame)) in zip(axes, key_events.items()):\n",
    "    # Use merged_monthly_data for all events\n",
    "    snapshot = merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "    \n",
    "    # Plotting the data for Residential\n",
    "    snapshot.plot(column='residential_percent_change_from_baseline', ax=ax, legend=True, cmap='coolwarm', \n",
    "                  missing_kwds={'color': 'white'}, legend_kwds={'label': event})\n",
    "    ax.set_title(event)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088486ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the min and max for each event for Residential mobility\n",
    "\n",
    "# Looping through each key event to identify the top and bottom changes in mobility\n",
    "for event, (freq, time_frame) in key_events.items():\n",
    "    print(f\"\\nMost and Least Changes for {event} ({time_frame}):\")\n",
    "    \n",
    "    # Determine the relevant snapshot based on frequency\n",
    "    snapshot = merged_monthly_data[merged_monthly_data['week'] == time_frame] if freq == 'week' else merged_monthly_data[merged_monthly_data['month'] == time_frame]\n",
    "\n",
    "    # Drop rows with NaN values in the change column\n",
    "    snapshot = snapshot.dropna(subset=['residential_percent_change_from_baseline'])\n",
    "\n",
    "    # Identify the top 10 and bottom 10 changes\n",
    "    most_changed = snapshot.nlargest(10, 'residential_percent_change_from_baseline')\n",
    "    least_changed = snapshot.nsmallest(10, 'residential_percent_change_from_baseline')\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nCountries with the most positive changes:\")\n",
    "    print(most_changed[['name', 'residential_percent_change_from_baseline']])\n",
    "    \n",
    "    print(\"\\nCountries with the most negative changes:\")\n",
    "    print(least_changed[['name', 'residential_percent_change_from_baseline']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff457d84",
   "metadata": {},
   "source": [
    "###### ------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a77ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_copy with the shapefile to add continent information\n",
    "df_copy = pd.merge(df_copy, world[['name', 'continent']], how=\"left\", left_on=\"country_region\", right_on=\"name\")\n",
    "\n",
    "# Drop unnecessary columns if needed\n",
    "df_copy.drop(columns=['name'], inplace=True)\n",
    "\n",
    "# Check the result\n",
    "df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis from a different perspective: continent level\n",
    "\n",
    "#Create a new year column\n",
    "df_copy['year'] = pd.to_datetime(df_copy['date']).dt.year\n",
    "\n",
    "# List of mobility variables\n",
    "mobility_variables = ['workplaces_percent_change_from_baseline','retail_and_recreation_percent_change_from_baseline','grocery_and_pharmacy_percent_change_from_baseline','parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline','residential_percent_change_from_baseline']\n",
    "\n",
    "# Filter data for years 2020, 2021, 2022\n",
    "df_filtered = df_copy[df_copy['year'].isin([2020, 2021, 2022])]\n",
    "\n",
    "# Create a new column combining year and month for the x-axis label\n",
    "df_filtered['year_month'] = df_filtered['year'].astype(str) + '-' + df_filtered['month'].astype(str).str.zfill(2)\n",
    "\n",
    "#calculate the mean of the mobility variables\n",
    "df_line = df_filtered.groupby(['continent', 'year_month'])[mobility_variables].mean().reset_index()\n",
    "\n",
    "# Create a color palette for each variable\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "# Set up subplots for each continent\n",
    "continents = df_line['continent'].unique()\n",
    "fig, axs = plt.subplots(3, 2, figsize=(14, 25))  # 6 continents, 3x2 grid\n",
    "axs = axs.flatten()\n",
    "handles = []\n",
    "\n",
    "# Loop through each continent and plot the line graph\n",
    "for i, continent in enumerate(continents):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    #For continent:\n",
    "    df_continent = df_line[df_line['continent'] == continent]\n",
    "    \n",
    "    # Plot each mobility variable as a line\n",
    "    for j, variable in enumerate(mobility_variables):\n",
    "        line, = ax.plot(df_continent['year_month'], df_continent[variable], marker='o', label=variable, color=colors[j])\n",
    "        if i == 0: \n",
    "            handles.append(line)\n",
    "    \n",
    "    #baseline dotted line at y=0\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    \n",
    "    #\n",
    "    ax.set_title(continent, fontsize=14)\n",
    "    ax.set_xlabel('Year-Month')\n",
    "    ax.set_ylabel('Mobility Change (%)')\n",
    "       \n",
    "    ax.grid(True)\n",
    "    ax.set_xticks(df_continent['year_month'][::3])  \n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "#legend outside of the subplots\n",
    "fig.legend(handles, mobility_variables, loc='upper right', bbox_to_anchor=(1.15, 1), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Monthly Mobility Changes Across Continents (2020-2022)', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320fafc",
   "metadata": {},
   "source": [
    "## K-means Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61364790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means clustering based on key events (all mobility combined) - determining number of clusters\n",
    "#Reference: (GeeksforGeeks, 2019)\n",
    "#Reference: (Kumar, 2020)\n",
    "\n",
    "#The time periods\n",
    "time_periods = {\n",
    "    \"WHO Declaration\": (\"2020-03-11\", \"2020-03-11\"),\n",
    "    \"First Lockdown\": (\"2020-03-01\", \"2020-05-31\"),\n",
    "    \"Second Wave and Lockdown\": (\"2020-12-01\", \"2021-02-28\"),\n",
    "    \"Peak of the Delta Variant\": (\"2021-06-01\", \"2021-08-31\"),\n",
    "    \"Omicron Surge\": (\"2021-12-01\", \"2022-02-28\")}\n",
    "\n",
    "#Create copy of dataframe for clustering\n",
    "df_clustering = df_filtered.copy()\n",
    "\n",
    "#A new DataFrame to store means for each time period\n",
    "df_means = pd.DataFrame()\n",
    "\n",
    "#Mean mobility for each time period\n",
    "for period_name, (start_date, end_date) in time_periods.items():\n",
    "    period_data = df_clustering[(df_clustering['date'] >= start_date) & (df_clustering['date'] <= end_date)]\n",
    "    \n",
    "    #Data is grouped by country and mean values calculated of mobility variables for clustering\n",
    "    period_mean = period_data.groupby('country_region')[mobility_variables].mean().reset_index()\n",
    "    period_mean['time_period'] = period_name  \n",
    "    # Append to the main DataFrame\n",
    "    df_means = pd.concat([df_means, period_mean], ignore_index=True)\n",
    "\n",
    "#Rows with NaN values removed in the mobility variables\n",
    "df_means = df_means.dropna()\n",
    "\n",
    "#Average mobility per country\n",
    "average_mobility = df_means.groupby('country_region')[mobility_variables].mean().reset_index()\n",
    "\n",
    "#Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(average_mobility[mobility_variables])\n",
    "\n",
    "#Elbow method to dind the optimal number of clusters (Elbow Method)\n",
    "#Empty list to store inertia values\n",
    "inertia = []\n",
    "# Define the range of k values from 1 to 10\n",
    "K = range(1, 11)\n",
    "#Looping through each value of K\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    #K means\n",
    "    kmeans.fit(X_scaled)\n",
    "    #Inertia valued added to the list created earlier\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "#Plot to visualise relationship between number of clusters and inertia\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K, inertia, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method to Determine Optimal k')\n",
    "plt.show()\n",
    "\n",
    "#silhouette scores\n",
    "silhouette_scores = []\n",
    "#Loop through K values\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    #Silhoutte scores calculated\n",
    "    score = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for Different k')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7aee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means clustering \n",
    "#Apply K-means clustering with the chosen number of clusters \n",
    "#The optimal number of clusters based on results was 3 hence \n",
    "optimal_k = 2\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "average_mobility['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "#Visualize the clusters using a pairplot\n",
    "sns.pairplot(average_mobility, hue='cluster', palette='Set2', plot_kws={'alpha': 0.5})\n",
    "plt.suptitle('Pairplot of Mobility Variables by Cluster', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#bar chart of the average mobility for each cluster\n",
    "cluster_avg = average_mobility.groupby('cluster')[mobility_variables].mean()\n",
    "\n",
    "cluster_avg.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Average Mobility Change by Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Average Percent Change from Baseline')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0dcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output countries in cluster\n",
    "\n",
    "#Output countries in cluster\n",
    "for cluster in average_mobility['cluster'].unique():\n",
    "    countries_in_cluster = average_mobility[average_mobility['cluster'] == cluster]['country_region'].values\n",
    "    print(f\"Countries in Cluster {cluster}: {countries_in_cluster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55031344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21d2bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#World Chloropleth map of clusters\n",
    "\n",
    "world = world.merge(average_mobility[['country_region', 'cluster']], how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "world.boundary.plot(ax=ax, linewidth=1)\n",
    "world.plot(column='cluster', ax=ax, legend=True, cmap='tab20',\n",
    "           missing_kwds={'color': 'lightgrey'}, legend_kwds={'label': \"Mobility Clusters\"})\n",
    "\n",
    "plt.title(\"Geospatial Clustering of Mobility Patterns Across Countries\", fontsize=16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line graph of clusters mobility over time avg\n",
    "\n",
    "#For loop to loop over clusters\n",
    "for cluster in average_mobility['cluster'].unique():\n",
    "#Obtain countries within the current cluster\n",
    "    countries_in_cluster = average_mobility[average_mobility['cluster'] == cluster]['country_region'].values\n",
    "    \n",
    "    # Filter the original DataFrame for these countries\n",
    "    cluster_data = df_filtered[df_filtered['country_region'].isin(countries_in_cluster)]\n",
    "    \n",
    "    # Group by date and calculate weekly means for each mobility variable\n",
    "    cluster_data.set_index('date', inplace=True)\n",
    "    weekly_data = cluster_data.resample('W')[mobility_variables].mean().reset_index()\n",
    "    \n",
    "    # Plot aggregate line graphs \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for var in mobility_variables:\n",
    "        plt.plot(weekly_data['date'], weekly_data[var], label=var)\n",
    "    \n",
    "    plt.title(f'Aggregate Mobility Variables for Cluster {cluster} Over Time (Weekly)', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Average Percent Change from Baseline', fontsize=14)\n",
    "    plt.legend(title='Mobility Variables', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbaa327",
   "metadata": {},
   "source": [
    "### Clustering for mobility variables individually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abd314",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# K-MEANS clustering for each mobility variable \n",
    "\n",
    "# Time periods for which clustering will be conducted\n",
    "time_periods = {\n",
    "    \"WHO Declaration\": (\"2020-03-11\", \"2020-03-11\"),\n",
    "    \"First Lockdown\": (\"2020-03-01\", \"2020-05-31\"),\n",
    "    \"Second Wave and Lockdown\": (\"2020-12-01\", \"2021-02-28\"),\n",
    "    \"Peak of the Delta Variant\": (\"2021-06-01\", \"2021-08-31\"),\n",
    "    \"Omicron Surge\": (\"2021-12-01\", \"2022-02-28\")}\n",
    "\n",
    "# Loop over each mobility variable to perform clustering individually\n",
    "for variable in mobility_variables:\n",
    "    \n",
    "    print(f\"Clustering for: {variable}\")\n",
    "    \n",
    "    # Prepare data for clustering (copy of df_filtered to avoid modifying original DataFrame)\n",
    "    df_variable_clustering = pd.DataFrame()\n",
    "\n",
    "    # Loop through each time period to calculate mean values\n",
    "    for period_name, (start_date, end_date) in time_periods.items():\n",
    "        period_data = df_filtered[(df_filtered['date'] >= start_date) & (df_filtered['date'] <= end_date)]\n",
    "        \n",
    "        # Group by country and calculate mean value of the mobility variable\n",
    "        period_mean = period_data.groupby('country_region')[variable].mean().reset_index()\n",
    "        period_mean['time_period'] = period_name  \n",
    "        \n",
    "        # Append to the DataFrame for clustering\n",
    "        df_variable_clustering = pd.concat([df_variable_clustering, period_mean], ignore_index=True)\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df_variable_clustering = df_variable_clustering.dropna()\n",
    "\n",
    "    # Standardise the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_variable_clustering[variable].values.reshape(-1, 1))\n",
    "\n",
    "    ### Find the optimal number of clusters ###\n",
    "    \n",
    "    # Elbow Method\n",
    "    inertia = []\n",
    "    K = range(1, 11)\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(K, inertia, marker='o')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title(f'Elbow Method to Determine Optimal k for {variable}')\n",
    "    plt.show()\n",
    "\n",
    "    # Silhouette Scores\n",
    "    silhouette_scores = []\n",
    "    for k in range(2, 11):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        score = silhouette_score(X_scaled, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title(f'Silhouette Scores for Different k for {variable}')\n",
    "    plt.show()\n",
    "\n",
    "    #optimal k based on the silhouette score and elbow method\n",
    "    optimal_k = 3  \n",
    "    \n",
    "    ### Apply K-means clustering ###\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    df_variable_clustering['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Calculate silhouette scores for each data point\n",
    "    sample_silhouette_values = silhouette_samples(X_scaled, df_variable_clustering['cluster'])\n",
    "    df_variable_clustering['silhouette_score'] = sample_silhouette_values\n",
    "\n",
    "    # Calculate the average silhouette score\n",
    "    avg_silhouette_score = silhouette_score(X_scaled, df_variable_clustering['cluster'])\n",
    "    print(f\"Average silhouette score for {variable}: {avg_silhouette_score:.4f}\")\n",
    "\n",
    "    # Identify the best fit country within each cluster for later analysis\n",
    "    best_fit_countries = df_variable_clustering.loc[df_variable_clustering.groupby('cluster')['silhouette_score'].idxmax(), ['country_region', 'cluster']]\n",
    "\n",
    "    # Print the best fit countries for each cluster\n",
    "    print(f\"Best fit countries for each cluster for {variable}:\")\n",
    "    print(best_fit_countries)\n",
    "\n",
    "    ### Calculate and Visualize Cluster Size and Distribution ###\n",
    "    \n",
    "    # Calculate the size of each cluster\n",
    "    cluster_size = df_variable_clustering['cluster'].value_counts().sort_index()\n",
    "    print(f\"Cluster sizes for {variable}:\")\n",
    "    print(cluster_size)\n",
    "\n",
    "    # Visualize the distribution of cluster sizes with custom colors\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=cluster_size.index, y=cluster_size.values, palette=['blue', 'red', 'green'])\n",
    "    plt.title(f\"Cluster Size Distribution for {variable}\")\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of Countries')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize the clusters for the single variable with custom colors\n",
    "    sns.histplot(data=df_variable_clustering, x=variable, hue='cluster', palette=['blue', 'red', 'green'], bins=20, alpha=0.7)\n",
    "    plt.title(f'Histogram of {variable} by Cluster')\n",
    "    plt.xlabel('Percent Change from Baseline')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    ### Plot the clusters on a world map ###\n",
    "    \n",
    "    # Load the world map shapefile using Geopandas\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "    # Merge clustering results with the world map\n",
    "    world = world.merge(df_variable_clustering[['country_region', 'cluster']], how=\"left\", left_on=\"name\", right_on=\"country_region\")\n",
    "\n",
    "    # Plot the clusters on the map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    world.boundary.plot(ax=ax, linewidth=1)\n",
    "\n",
    "    # Plot clusters, coloring countries based on their cluster assignment\n",
    "    world.plot(column='cluster', ax=ax, legend=True, cmap=ListedColormap(['blue', 'red', 'green']),\n",
    "               missing_kwds={'color': 'lightgrey'}, legend_kwds={'label': f\"{variable} Clusters\"})\n",
    "\n",
    "    plt.title(f\"Geospatial Clustering of {variable} Patterns Across Countries\", fontsize=16)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b12bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca68d85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0eaba6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Countries within each cluster and mobility variable\n",
    "\n",
    "# Define the time periods\n",
    "time_periods = {\n",
    "    \"WHO Declaration\": (\"2020-03-11\", \"2020-03-11\"),\n",
    "    \"First Lockdown\": (\"2020-03-01\", \"2020-05-31\"),\n",
    "    \"Second Wave and Lockdown\": (\"2020-12-01\", \"2021-02-28\"),\n",
    "    \"Peak of the Delta Variant\": (\"2021-06-01\", \"2021-08-31\"),\n",
    "    \"Omicron Surge\": (\"2021-12-01\", \"2022-02-28\")}\n",
    "\n",
    "# Loop over each mobility variable to examine clusters individually\n",
    "for variable in mobility_variables:\n",
    "    print(f\"--- Analysis for {variable} ---\")\n",
    "\n",
    "    for period_name, (start_date, end_date) in time_periods.items():\n",
    "        print(f\"\\nAnalyzing for the time period: {period_name}\")\n",
    "\n",
    "        period_data = df_filtered[(df_filtered['date'] >= start_date) & (df_filtered['date'] <= end_date)]\n",
    "\n",
    "        #I\n",
    "        df_variable_clustering = period_data[['country_region', variable]].copy()\n",
    "        df_variable_clustering = df_variable_clustering.groupby('country_region')[variable].mean().dropna()\n",
    "\n",
    "        # Standardise the data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(df_variable_clustering.values.reshape(-1, 1))\n",
    "\n",
    "        # Apply k-means clustering using K=3 \n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)  \n",
    "        cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "        # Add cluster labels to the DataFrame\n",
    "        df_variable_clustering = df_variable_clustering.to_frame()\n",
    "        df_variable_clustering['cluster'] = cluster_labels\n",
    "\n",
    "        # Display countries in each cluster\n",
    "        for cluster_id in df_variable_clustering['cluster'].unique():\n",
    "            print(f\"\\nCountries in Cluster {cluster_id} for {variable} during {period_name}:\")\n",
    "            cluster_countries = df_variable_clustering[df_variable_clustering['cluster'] == cluster_id]\n",
    "            print(cluster_countries.index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d2c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d2a1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Average mobility change by cluster\n",
    "\n",
    "# Define the time periods\n",
    "time_periods = {\n",
    "    \"WHO Declaration\": (\"2020-03-11\", \"2020-03-11\"),\n",
    "    \"First Lockdown\": (\"2020-03-01\", \"2020-05-31\"),\n",
    "    \"Second Wave and Lockdown\": (\"2020-12-01\", \"2021-02-28\"),\n",
    "    \"Peak of the Delta Variant\": (\"2021-06-01\", \"2021-08-31\"),\n",
    "    \"Omicron Surge\": (\"2021-12-01\", \"2022-02-28\")}\n",
    "\n",
    "#For loop to loop over each mobility variable to perform clustering individually\n",
    "for variable in mobility_variables:\n",
    "    \n",
    "    print(f\"Analyzing Clusters for: {variable}\")\n",
    "    \n",
    "    #New DataFrame to store results for clustering\n",
    "    df_variable_clustering = pd.DataFrame()\n",
    "\n",
    "    # Loop through each time period to calculate mean values\n",
    "    for period_name, (start_date, end_date) in time_periods.items():\n",
    "        period_data = df_filtered[(df_filtered['date'] >= start_date) & (df_filtered['date'] <= end_date)]\n",
    "        \n",
    "        # Group by country and calculate mean value of the mobility variable\n",
    "        period_mean = period_data.groupby('country_region')[variable].mean().reset_index()\n",
    "        period_mean['time_period'] = period_name \n",
    "        \n",
    "        # Append to the DataFrame for clustering\n",
    "        df_variable_clustering = pd.concat([df_variable_clustering, period_mean], ignore_index=True)\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df_variable_clustering = df_variable_clustering.dropna()\n",
    "\n",
    "    # Standardise the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_variable_clustering[variable].values.reshape(-1, 1))\n",
    "\n",
    "    # Apply K-means clustering using k=3 as this was determined to be the optimal number of clusters\n",
    "    optimal_k = 3 \n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    df_variable_clustering['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "    # Calculate silhouette scores for each data point\n",
    "    sample_silhouette_values = silhouette_samples(X_scaled, df_variable_clustering['cluster'])\n",
    "    df_variable_clustering['silhouette_score'] = sample_silhouette_values\n",
    "\n",
    "    # Calculate the average silhouette score\n",
    "    avg_silhouette_score = silhouette_score(X_scaled, df_variable_clustering['cluster'])\n",
    "    print(f\"Average silhouette score for {variable}: {avg_silhouette_score:.4f}\")\n",
    "\n",
    "    #Best fit country (highest silhouette score) within each cluster\n",
    "    best_fit_countries = df_variable_clustering.loc[df_variable_clustering.groupby('cluster')['silhouette_score'].idxmax(), ['country_region', 'cluster']]\n",
    "\n",
    "    \n",
    "    print(f\"Best fit countries for each cluster for {variable}:\")\n",
    "    print(best_fit_countries)\n",
    "\n",
    "    ### Analyze the cluster characteristics ###\n",
    "    \n",
    "    # Calculate descriptive statistics for each cluster\n",
    "    cluster_summary = df_variable_clustering.groupby('cluster')[variable].describe()\n",
    "    print(f\"Cluster summary for {variable}:\\n\", cluster_summary)\n",
    "\n",
    "    # Visualize the average value of the variable for each cluster with specified colors\n",
    "    cluster_means = df_variable_clustering.groupby('cluster')[variable].mean()\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=cluster_means.index, y=cluster_means.values, palette=['blue', 'red', 'green'])\n",
    "    plt.title(f\"Average {variable} by Cluster\")\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(f'Average {variable} Change from Baseline (%)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize distributions for each cluster\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='cluster', y=variable, data=df_variable_clustering, palette=['blue', 'red', 'green'])\n",
    "    plt.title(f\"Distribution of {variable} within Clusters\")\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(f'{variable} Change from Baseline (%)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17380377",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd268ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal component analysis\n",
    "#Reference: (GeeksforGeeks, 2018)\n",
    "\n",
    "mobility_variables = ['workplaces_percent_change_from_baseline','retail_and_recreation_percent_change_from_baseline','grocery_and_pharmacy_percent_change_from_baseline','parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline','residential_percent_change_from_baseline']\n",
    "\n",
    "# Drop NaN values\n",
    "df_clustering = df_clustering.dropna(subset=mobility_variables)\n",
    "\n",
    "#Standardise the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_clustering[mobility_variables])\n",
    "\n",
    "#Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "#Add PCA components to the DataFrame\n",
    "df_clustering['PCA1'] = X_pca[:, 0]\n",
    "df_clustering['PCA2'] = X_pca[:, 1]\n",
    "\n",
    "# Apply K-means clustering \n",
    "optimal_k = 2 \n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "df_clustering['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "#Plot the PCA results with clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_clustering, x='PCA1', y='PCA2', hue='cluster', palette='Set2', s=100, alpha=0.7)\n",
    "plt.title(\"PCA Visualization of Clusters Across All Variables\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604aabcf",
   "metadata": {},
   "source": [
    "### G20 and LDC - Country v Country level analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bfd72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d0d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c101ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#line graph comparison G20 and LDC\n",
    "\n",
    "df_copy['year'] = df_copy['date'].dt.year\n",
    "df_copy['month'] = df_copy['date'].dt.month\n",
    "\n",
    "# List of G20 countries from your dataset (without China)\n",
    "g20_countries_present = [\"Argentina\", \"Australia\", \"Brazil\", \"Canada\", \"France\", \"Germany\", \"India\", \"Indonesia\", \"Italy\", \"Japan\", \"Mexico\", \"Russia\", \"Saudi Arabia\", \"South Africa\", \"South Korea\", \"Turkey\", \"United Kingdom\", \"United States\"]\n",
    "\n",
    "# List of LDC countries from your dataset\n",
    "ldc_countries_present = [\"Afghanistan\", \"Angola\", \"Bangladesh\", \"Burkina Faso\", \"Haiti\", \"Zambia\", \"Mali\", \"Mozambique\", \"Niger\", \"Rwanda\", \"Senegal\", \"Tanzania\", \"Uganda\", \"Yemen\"]\n",
    "\n",
    "# Average monthly averages and variance for G20 and LDC groups\n",
    "g20_avg = df_copy[df_copy['country_region'].isin(g20_countries_present)].groupby(['year', 'month'])[mobility_variables].agg(['mean', 'std']).reset_index()\n",
    "ldc_avg = df_copy[df_copy['country_region'].isin(ldc_countries_present)].groupby(['year', 'month'])[mobility_variables].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Combine year and month to create a time-based x-axis\n",
    "g20_avg['time'] = pd.to_datetime(g20_avg['year'].astype(str) + '-' + g20_avg['month'].astype(str) + '-01')\n",
    "ldc_avg['time'] = pd.to_datetime(ldc_avg['year'].astype(str) + '-' + ldc_avg['month'].astype(str) + '-01')\n",
    "\n",
    "# Visualization\n",
    "for variable in mobility_variables:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # G20 plot with standard deviation\n",
    "    plt.plot(g20_avg['time'], g20_avg[(variable, 'mean')], marker='o', label='G20 Average')\n",
    "    plt.fill_between(g20_avg['time'], \n",
    "                     g20_avg[(variable, 'mean')] - g20_avg[(variable, 'std')],\n",
    "                     g20_avg[(variable, 'mean')] + g20_avg[(variable, 'std')],\n",
    "                     color='blue', alpha=0.2)  # Shaded area for G20\n",
    "    \n",
    "    # LDC plot with standard deviation\n",
    "    plt.plot(ldc_avg['time'], ldc_avg[(variable, 'mean')], marker='o', label='LDC Average')\n",
    "    plt.fill_between(ldc_avg['time'], \n",
    "                     ldc_avg[(variable, 'mean')] - ldc_avg[(variable, 'std')],\n",
    "                     ldc_avg[(variable, 'mean')] + ldc_avg[(variable, 'std')],\n",
    "                     color='red', alpha=0.2)  # Shaded area for LDC\n",
    "\n",
    "    plt.title(f'Average Mobility for {variable} (G20 vs. LDC)')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'{variable} (% change from baseline)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9650f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81828b7b",
   "metadata": {},
   "source": [
    "### Mobility pattern analysis extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset obtained from https://ourworldindata.org/explorers/covid\n",
    "#Reference: (Our World in Data, 2022)\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('owid-covid-data.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7519513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Country name adjustment\n",
    "#identify unique country names from df_copy and d1 (the new dataframe)\n",
    "df_copy_countries = df_copy['country_region'].unique()\n",
    "df1_countries = df1['location'].unique()\n",
    "\n",
    "#Find mismatched country names\n",
    "countries_not_in_df1 = set(df_copy_countries) - set(df1_countries)\n",
    "countries_not_in_df_copy = set(df1_countries) - set(df_copy_countries)\n",
    "\n",
    "#Display the results\n",
    "print(\"Countries in `df_copy` not found in `df1`:\")\n",
    "print(countries_not_in_df1)\n",
    "\n",
    "print(\"\\nCountries in `df1` not found in `df_copy`:\")\n",
    "print(countries_not_in_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ec1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping to match country names from `df1` to `df_copy`\n",
    "owid_to_df_copy_mapping = {\n",
    "    \"Cote d'Ivoire\": \"Côte d'Ivoire\",\n",
    "    \"Antigua and Barbuda\": \"Antigua\",\n",
    "    \"Cape Verde\": \"Cabo Verde\",\n",
    "    \"Dominican Republic\": \"Dominican Rep.\",\n",
    "    \"Bosnia and Herzegovina\": \"Bosnia and Herz.\",\n",
    "    \"United States\": \"United States of America\"}\n",
    "\n",
    "# Apply the country name mapping to `df1`\n",
    "df1['location'] = df1['location'].replace(owid_to_df_copy_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba917c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding continent data to df_copy using shapefile\n",
    "#Shapefile containing continent data\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Merge df_copy with the shapefile to add continent information\n",
    "df_copy = pd.merge(df_copy, world[['name', 'continent']], how=\"left\", left_on=\"country_region\", right_on=\"name\")\n",
    "\n",
    "# Drop unnecessary columns if needed\n",
    "df_copy.drop(columns=['name'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b52bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f935d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of `df_copy` for further adjustment\n",
    "df_copy1 = df_copy.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389a960",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Adjusting dataframe to extend till 2024 for predictive modelling\n",
    "\n",
    "if 'continent' not in df_copy1.columns:\n",
    "    # Add a 'continent' column with placeholder values if it's missing\n",
    "    df_copy1['continent'] = pd.NA\n",
    "\n",
    "\n",
    "#We want to keep rows in `df1` that match the countries in `df_copy1`\n",
    "relevant_countries = df_copy1['country_region'].unique()\n",
    "df1_filtered = df1[df1['location'].isin(relevant_countries)]\n",
    "\n",
    "#Make`date` columns are in datetime format\n",
    "df_copy1['date'] = pd.to_datetime(df_copy1['date'])\n",
    "df1_filtered['date'] = pd.to_datetime(df1_filtered['date'])\n",
    "\n",
    "#Filter `df1` for dates after `2022-10-13` up to `2024-08-04`\n",
    "future_dates = df1_filtered[(df1_filtered['date'] > '2022-10-13') & (df1_filtered['date'] <= '2024-08-04')]\n",
    "\n",
    "#Empty list to store extended rows\n",
    "extended_rows_list = []\n",
    "\n",
    "#Extend data for each country in `df_copy1`using for loop\n",
    "for country in relevant_countries:\n",
    "    # Filter future rows for the specific country\n",
    "    future_data = future_dates[future_dates['location'] == country]\n",
    "\n",
    "    # Find continent information from `df_copy1` or set as `NaN` if not found\n",
    "    if not df_copy1[df_copy1['country_region'] == country].empty:\n",
    "        continent_value = df_copy1[df_copy1['country_region'] == country]['continent'].iloc[0]\n",
    "    else:\n",
    "        continent_value = pd.NA\n",
    "\n",
    "    #The relevant columns and add placeholders for `df_copy1` columns\n",
    "    extended_rows = pd.DataFrame({\n",
    "        'country_region_code': pd.NA,  \n",
    "        'country_region': future_data['location'],\n",
    "        'date': future_data['date'],\n",
    "        'retail_and_recreation_percent_change_from_baseline': pd.NA,\n",
    "        'grocery_and_pharmacy_percent_change_from_baseline': pd.NA,\n",
    "        'parks_percent_change_from_baseline': pd.NA,\n",
    "        'transit_stations_percent_change_from_baseline': pd.NA,\n",
    "        'workplaces_percent_change_from_baseline': pd.NA,\n",
    "        'residential_percent_change_from_baseline': pd.NA,\n",
    "        'year': future_data['date'].dt.year,\n",
    "        'week': future_data['date'].dt.isocalendar().week,\n",
    "        'month': future_data['date'].dt.month,\n",
    "        'continent': continent_value,  \n",
    "        'human_development_index': future_data['human_development_index'],\n",
    "        'population': future_data['population']})\n",
    "    \n",
    "# Append the new rows for each country\n",
    "    extended_rows_list.append(extended_rows)\n",
    "\n",
    "# Combine all extended rows into a single DataFrame\n",
    "extended_rows_df = pd.concat(extended_rows_list, ignore_index=True)\n",
    "\n",
    "#Append the extended rows to `df_copy1`\n",
    "df_copy1_extended = pd.concat([df_copy1, extended_rows_df], ignore_index=True)\n",
    "\n",
    "#The new dataframe\n",
    "df_copy1_extended \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link to dataset: https://data.who.int/dashboards/covid19/data?n=o\n",
    "#Reference: (datadot, 2019)\n",
    "\n",
    "# Import the WHO dataset\n",
    "who_file_path = 'WHO-COVID-19-global-daily-data.csv'\n",
    "df_who = pd.read_csv(who_file_path)\n",
    "\n",
    "# Display \n",
    "df_who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line graph to visualise new cases against cumulative cases\n",
    "#Reference: (Holtz, 2019)\n",
    "\n",
    "# Convert the 'Date_reported' column to datetime format\n",
    "df_who['Date_reported'] = pd.to_datetime(df_who['Date_reported'])\n",
    "\n",
    "# Aggregating the data by date across all countries for new and cumulative cases\n",
    "df_who_aggregated = df_who.groupby('Date_reported').agg({'New_cases': 'sum','Cumulative_cases': 'sum'}).reset_index()\n",
    "\n",
    "# Setting 'Date_reported' as the index for plotting\n",
    "df_who_aggregated.set_index('Date_reported', inplace=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plotting New Cases\n",
    "ax1 = plt.gca() \n",
    "ax1.plot(df_who_aggregated.index, df_who_aggregated['New_cases'], label='New Cases', color='blue', linestyle='-', marker='o')\n",
    "ax1.set_ylabel('New Cases', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Create a second y-axis for Cumulative Cases\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_who_aggregated.index, df_who_aggregated['Cumulative_cases'], label='Cumulative Cases', color='red', linestyle='-', marker='x')\n",
    "ax2.set_ylabel('Cumulative Cases', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "\n",
    "plt.title('Aggregated New Cases and Cumulative Cases Over Time (All Countries)')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax1.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#country name mapping for WHO dataset\n",
    "\n",
    "#Extract unique country names from WHO and df_copy1\n",
    "who_countries = df_who['Country'].unique()\n",
    "df_copy1_countries = df_copy1_extended['country_region'].unique()\n",
    "\n",
    "#Find mismatched country names\n",
    "countries_not_in_copy1 = set(who_countries) - set(df_copy1_countries)\n",
    "countries_not_in_who = set(df_copy1_countries) - set(who_countries)\n",
    "\n",
    "#Display the results\n",
    "print(\"Countries in WHO data not found in `df_copy1_extended`:\")\n",
    "print(countries_not_in_copy1)\n",
    "\n",
    "print(\"\\nCountries in `df_copy1_extended` not found in WHO data:\")\n",
    "print(countries_not_in_who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0867ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually Mapping for WHO country names to align with `df_copy1_extended`\n",
    "who_name_mapping = {\n",
    "    'United Republic of Tanzania': 'Tanzania',\n",
    "    'Türkiye': 'Turkey',\n",
    "    'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom',\n",
    "    'Republic of Korea': 'South Korea',\n",
    "    'Venezuela (Bolivarian Republic of)': 'Venezuela',\n",
    "    'Dominican Republic': 'Dominican Rep.',\n",
    "    'Russian Federation': 'Russia',\n",
    "    'Republic of Moldova': 'Moldova',\n",
    "    'Viet Nam': 'Vietnam',\n",
    "    \"Lao People's Democratic Republic\": 'Laos',\n",
    "    'Bolivia (Plurinational State of)': 'Bolivia',\n",
    "    'Antigua and Barbuda': 'Antigua',\n",
    "    'Bosnia and Herzegovina': 'Bosnia and Herz.'}\n",
    "\n",
    "# Apply the mapping to standardize WHO country names\n",
    "df_who['Country'] = df_who['Country'].replace(who_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3eb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging cumulative cases and new cases column with df_copy1_extended\n",
    "\n",
    "# Rename the `Date_reported` column to `date` for easier merging\n",
    "df_who.rename(columns={'Date_reported': 'date'}, inplace=True)\n",
    "\n",
    "# Filter the WHO dataset for dates up to `2024-08-04`\n",
    "df_who_filtered = df_who[df_who['date'] <= '2024-08-04']\n",
    "\n",
    "# Merge WHO data with df_copy1_extended to add 'New_cases' and 'Cumulative_cases'\n",
    "df_copy1_extended = df_copy1_extended.merge(\n",
    "    df_who_filtered[['Country', 'date', 'New_cases', 'Cumulative_cases']],how='left',left_on=['country_region', 'date'],right_on=['Country', 'date'])\n",
    "\n",
    "# Drop the redundant 'Country' column from the WHO dataset\n",
    "df_copy1_extended.drop(columns=['Country'], inplace=True)\n",
    "\n",
    "\n",
    "df_copy1_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "##link to dataset: https://ourworldindata.org/covid-stay-home-restrictions\n",
    "#Stay at home dataset\n",
    "#Reference: (Mathieu et al., 2020)\n",
    "df2 = pd.read_csv('stay-at-home-covid.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matching df2 country names\n",
    "\n",
    "df2['Day'] = pd.to_datetime(df2['Day'])  \n",
    "\n",
    "df2_countries = df2['Entity'].unique()\n",
    "\n",
    "unmatched_countries = set(df2['Entity']).difference(world['name'])\n",
    "print(\"Countries in df2 not found in shapefile:\", unmatched_countries)\n",
    "\n",
    "world_countries = world['name'].unique()\n",
    "world_countries\n",
    "\n",
    "\n",
    "countries_not_in_df2 = set(world_countries) - set(df2_countries)\n",
    "print(\"Countries in shapefile not found in df2:\", countries_not_in_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9faea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping for country names in `df2` to match those in the shapefile\n",
    "df2_to_shapefile_mapping = {\n",
    "    'United States Virgin Islands': 'Virgin Islands',\n",
    "    'Macao': 'Macau',\n",
    "    'United States': 'United States of America',\n",
    "    \"Cote d'Ivoire\": \"Côte d'Ivoire\", \n",
    "    'East Timor': 'Timor-Leste',\n",
    "    'Eswatini': 'eSwatini',\n",
    "    'Cabo Verde': 'Cape Verde',\n",
    "    'Democratic Republic of Congo': 'Dem. Rep. Congo',\n",
    "    'Solomon Islands': 'Solomon Is.',\n",
    "    'Central African Republic': 'Central African Rep.',\n",
    "    'Dominican Republic': 'Dominican Rep.',\n",
    "    'South Sudan': 'S. Sudan',\n",
    "    'Bosnia and Herzegovina': 'Bosnia and Herz.'}\n",
    "\n",
    "#Apply mapping\n",
    "df2['Entity'] = df2['Entity'].replace(df2_to_shapefile_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b3044",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Chloropleth map of stay at home restrictions over key periods.\n",
    "\n",
    "\n",
    "# Group data by country and day, keeping the latest stay-at-home requirement per day\n",
    "stay_home_data = df2.groupby(['Entity', 'Day'])['Stay at home requirements'].last().reset_index()\n",
    "\n",
    "# Define the time periods\n",
    "time_periods = {\n",
    "    \"WHO Declaration\": (\"2020-03-11\", \"2020-03-11\"),\n",
    "    \"First Lockdown\": (\"2020-03-01\", \"2020-05-31\"),\n",
    "    \"Second Wave and Lockdown\": (\"2020-12-01\", \"2021-02-28\"),\n",
    "    \"Peak of the Delta Variant\": (\"2021-06-01\", \"2021-08-31\"),\n",
    "    \"Omicron Surge\": (\"2021-12-01\", \"2022-02-28\"),\n",
    "    \"End of Data\": (\"2022-12-01\", \"2022-12-31\")  }\n",
    "\n",
    "# Load the world shapefile from Geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Create plots for each time period\n",
    "for period_name, (start_date, end_date) in time_periods.items():\n",
    "    print(f\"Plotting for period: {period_name}\")\n",
    "    # Filter data for the specified time period\n",
    "    period_data = stay_home_data[(stay_home_data['Day'] >= start_date) & (stay_home_data['Day'] <= end_date)]\n",
    "    world_data = world.merge(period_data, left_on='name', right_on='Entity', how='left')\n",
    "\n",
    "    # Plot the choropleth map\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    world_data.boundary.plot(ax=ax, linewidth=1)\n",
    "    world_data.plot(column='Stay at home requirements', ax=ax, legend=True,cmap='OrRd', missing_kwds={'color': 'lightgrey'},legend_kwds={'label': \"Stay-at-Home Requirements\",'orientation': \"horizontal\"})\n",
    "\n",
    "\n",
    "    plt.title(f'Stay-at-Home Requirements during {period_name}', fontsize=16)\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.axis('off') \n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7012c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping with original dataframe\n",
    "# Extract unique countries from both dataframes\n",
    "unique_countries_df2 = set(df2['Entity'].unique())\n",
    "unique_countries_df_copy1_extended = set(df_copy1_extended['country_region'].unique())\n",
    "\n",
    "# Find common countries\n",
    "common_countries = unique_countries_df2.intersection(unique_countries_df_copy1_extended)\n",
    "\n",
    "# Find countries unique to each dataframe\n",
    "unique_to_df2 = unique_countries_df2 - unique_countries_df_copy1_extended\n",
    "unique_to_df_copy1_extended = unique_countries_df_copy1_extended - unique_countries_df2\n",
    "\n",
    "# Print results\n",
    "print(\"Common countries:\", common_countries )\n",
    "\n",
    "print(\"Countries unique to df2:\", unique_to_df2)\n",
    "print(\"Countries unique to df_copy1_extended:\", unique_to_df_copy1_extended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13451bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping to match country names between df2 and df_copy1_extended for comparison\n",
    "name_mapping = {\n",
    "    \"United States\": \"United States of America\",\n",
    "    \"Cote d'Ivoire\": \"Côte d'Ivoire\",\n",
    "    \"Bosnia and Herzegovina\": \"Bosnia and Herz.\",\n",
    "    \"Guinea-Bissau\": \"Guinea-Bissau\",\n",
    "    \"Cabo Verde\": \"Cape Verde\",\n",
    "    \"Democratic Republic of Congo\": \"Democratic Republic of Congo\", \n",
    "    \"Dominican Republic\": \"Dominican Rep.\",}\n",
    "\n",
    "df2['Mapped_Entity'] = df2['Entity'].replace(name_mapping)\n",
    "df_copy1_extended['Mapped_country_region'] = df_copy1_extended['country_region']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68108694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding stay at home requirements column to df_copy1_extended\n",
    "df2['Day'] = pd.to_datetime(df2['Day'])\n",
    "df_copy1_extended['date'] = pd.to_datetime(df_copy1_extended['date'])\n",
    "df_copy1_extended = df_copy1_extended.merge(\n",
    "    df2[['Entity', 'Day', 'Stay at home requirements']],\n",
    "    how='left',\n",
    "    left_on=['country_region', 'date'],\n",
    "    right_on=['Entity', 'Day'])\n",
    "\n",
    "df_copy1_extended.drop(columns=['Entity', 'Day'], inplace=True)\n",
    "\n",
    "#Renaming \"Stay at home requirements\" column as needed stay_at_home_requirements\n",
    "df_copy1_extended.rename(columns={'Stay at home requirements': 'stay_at_home_requirements'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d82a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of avg mobility and stay at home requirements\n",
    "\n",
    "# Convert 'date' column to datetime if not already in that format\n",
    "\n",
    "df_copy1_extended['month_year'] = df_copy1_extended['date'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# List of mobility variables\n",
    "mobility_variables = [\n",
    "    'retail_and_recreation_percent_change_from_baseline',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline',\n",
    "    'transit_stations_percent_change_from_baseline',\n",
    "    'workplaces_percent_change_from_baseline',\n",
    "    'residential_percent_change_from_baseline']\n",
    "\n",
    "# Group by month and stay-at-home requirements to calculate monthly averages\n",
    "df_monthly_avg = df_copy1_extended.groupby(['month_year', 'stay_at_home_requirements'])[mobility_variables].mean().reset_index()\n",
    "\n",
    "# Pivot the data for each variable to create a heatmap\n",
    "for variable in mobility_variables:\n",
    "    pivot_table = df_monthly_avg.pivot(index='month_year', columns='stay_at_home_requirements', values=variable)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.heatmap(pivot_table, cmap='viridis', annot=True, fmt=\".1f\", cbar_kws={'label': '% Change from Baseline'})\n",
    "    plt.title(f\"Monthly Averages of {variable} vs Stay-at-Home Requirements\")\n",
    "    plt.xlabel(\"Stay-at-Home Requirement\")\n",
    "    plt.ylabel(\"Month-Year\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c596e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Linw graph of stay at home requirements over time\n",
    "\n",
    "\n",
    "\n",
    "# List of mobility variables\n",
    "mobility_variables = [\n",
    "    'retail_and_recreation_percent_change_from_baseline',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline',\n",
    "    'transit_stations_percent_change_from_baseline',\n",
    "    'workplaces_percent_change_from_baseline',\n",
    "    'residential_percent_change_from_baseline']\n",
    "\n",
    "# Group by month and stay-at-home requirements to calculate monthly averages\n",
    "df_monthly_avg = df_copy1_extended.groupby(['month_year', 'stay_at_home_requirements'])[mobility_variables].mean().reset_index()\n",
    "\n",
    "#Line plots for each mobility variable\n",
    "for variable in mobility_variables:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_monthly_avg, x='month_year', y=variable, hue='stay_at_home_requirements', palette='viridis', marker='o')\n",
    "    plt.title(f\"Change in {variable} with Stay-at-Home Requirements Over Time\")\n",
    "    plt.xlabel(\"Month-Year\")\n",
    "    plt.ylabel(f\"{variable} (% change from baseline)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Stay-at-Home Requirement Level\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876f586",
   "metadata": {},
   "source": [
    "### Hierarchiel Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d0b76",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Hierarchiel Clustering\n",
    "#Reference: (my, 2020)\n",
    "\n",
    "\n",
    "#key time periods\n",
    "time_periods = {\n",
    "    \"WHO Declaration\": (\"2020-03-11\", \"2020-03-11\"),\n",
    "    \"First Lockdown\": (\"2020-03-01\", \"2020-05-31\"),\n",
    "    \"Second Wave and Lockdown\": (\"2020-12-01\", \"2021-02-28\"),\n",
    "    \"Peak of the Delta Variant\": (\"2021-06-01\", \"2021-08-31\"),\n",
    "    \"Omicron Surge\": (\"2021-12-01\", \"2022-02-28\")}\n",
    "\n",
    "# Function to prepare data and aggregate by country and time period\n",
    "def prepare_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['time_period'] = None\n",
    "\n",
    "    for period_name, (start_date, end_date) in time_periods.items():\n",
    "        mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
    "        df.loc[mask, 'time_period'] = period_name\n",
    "    \n",
    "    # Filter out rows without a defined time period\n",
    "    df = df[df['time_period'].notna()]\n",
    "\n",
    "    # Group by country to get averages\n",
    "    aggregated_data = df.groupby(['country_region']).agg(\n",
    "        avg_stay_home=('stay_at_home_requirements', 'mean'),\n",
    "        sum_new_cases=('New_cases', 'mean') \n",
    "    ).reset_index()\n",
    "\n",
    "    return aggregated_data\n",
    "\n",
    "#Prepare the data\n",
    "weekly_country_data = prepare_data(df_copy1_extended)\n",
    "\n",
    "#Handle NaNs values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "features = imputer.fit_transform(weekly_country_data[['avg_stay_home', 'sum_new_cases']])\n",
    "\n",
    "#Normalise the data for clustering\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "#Perform hierarchical clustering globally\n",
    "linkage_matrix = linkage(features_scaled, method='ward')\n",
    "\n",
    "#Plot the global dendrogram to identify significant clusters\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(linkage_matrix,labels=weekly_country_data['country_region'].values,leaf_rotation=90,leaf_font_size=8)\n",
    "plt.title('Global Dendrogram for Hierarchical Clustering of All Countries (Key Time Periods)')\n",
    "plt.xlabel('Countries')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "#Identify significant clusters (e.g., based on a threshold distance)\n",
    "threshold_distance = 10  \n",
    "clusters = fcluster(linkage_matrix, t=threshold_distance, criterion='distance')\n",
    "\n",
    "# Add cluster labels to the data\n",
    "weekly_country_data['cluster'] = clusters\n",
    "\n",
    "#Create line graphs for each cluster aggregated by country for stay-at-home restrictions\n",
    "df_copy1_extended['date'] = pd.to_datetime(df_copy1_extended['date'])\n",
    "\n",
    "# Get the start date of each week (week beginning date)\n",
    "df_copy1_extended['week_start'] = df_copy1_extended['date'] - pd.to_timedelta(df_copy1_extended['date'].dt.weekday, unit='D')\n",
    "\n",
    "# Create a line graph for each cluster\n",
    "for cluster in weekly_country_data['cluster'].unique():\n",
    "    countries_in_cluster = weekly_country_data[weekly_country_data['cluster'] == cluster]['country_region']\n",
    "    cluster_data = df_copy1_extended[df_copy1_extended['country_region'].isin(countries_in_cluster)]\n",
    "    \n",
    "    # Aggregate data by week for stay-at-home restrictions\n",
    "    weekly_data = cluster_data.groupby(['week_start']).agg(avg_stay_home=('stay_at_home_requirements', 'mean'),).reset_index()\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add stay-at-home restriction trace\n",
    "    fig.add_trace(go.Scatter(x=weekly_data['week_start'], y=weekly_data['avg_stay_home'],mode='lines+markers',name=f'Avg Stay-at-Home - Cluster {cluster}',line=dict(color='blue')))\n",
    "\n",
    "    fig.update_layout(title=f'Weekly Avg Stay-at-Home Restrictions for Cluster {cluster}',xaxis_title='Week Beginning Date',yaxis_title='Avg Stay-at-Home Restriction Level',height=600)\n",
    "\n",
    "    # Display the figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786205c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Linear regression for mobility variables\n",
    "#Reference: (scikit-learn, 2024)\n",
    "\n",
    "# Prepare the data for clustering\n",
    "def prepare_data(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['week_start'] = df['date'] - pd.to_timedelta(df['date'].dt.weekday, unit='D')\n",
    "    \n",
    "    aggregated_data = df.groupby(['country_region', 'week_start']).agg(retail=('retail_and_recreation_percent_change_from_baseline', 'mean'),grocery=('grocery_and_pharmacy_percent_change_from_baseline', 'mean'),parks=('parks_percent_change_from_baseline', 'mean'),transit=('transit_stations_percent_change_from_baseline', 'mean'),workplaces=('workplaces_percent_change_from_baseline', 'mean'),residential=('residential_percent_change_from_baseline', 'mean'),new_cases=('New_cases', 'sum')).reset_index()\n",
    "    \n",
    "    return aggregated_data\n",
    "\n",
    "# Prepare the data\n",
    "weekly_country_data = prepare_data(df_copy1_extended)\n",
    "\n",
    "# Normalise data for clustering\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "features = imputer.fit_transform(weekly_country_data[['retail', 'grocery', 'parks', 'transit', 'workplaces', 'residential', 'new_cases']])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(features_scaled, method='ward')\n",
    "\n",
    "# Identify significant clusters, aiming for 3 clusters\n",
    "clusters = fcluster(linkage_matrix, t=3, criterion='maxclust')\n",
    "\n",
    "# Add cluster labels to the data\n",
    "weekly_country_data['cluster'] = clusters\n",
    "\n",
    "# Predict future mobility variables based on historical data\n",
    "future_dates = pd.date_range(start='2022-10-14', end='2024-08-04', freq='W-MON')\n",
    "predictions = {}\n",
    "\n",
    "# Define a color map for mobility variables\n",
    "color_map = {\n",
    "    'retail': 'blue',\n",
    "    'grocery': 'green',\n",
    "    'parks': 'orange',\n",
    "    'transit': 'purple',\n",
    "    'workplaces': 'brown',\n",
    "    'residential': 'pink'}\n",
    "\n",
    "# Loop through each cluster\n",
    "for cluster in np.unique(clusters):\n",
    "    countries_in_cluster = weekly_country_data[weekly_country_data['cluster'] == cluster]['country_region'].unique()\n",
    "    cluster_data = weekly_country_data[weekly_country_data['country_region'].isin(countries_in_cluster)]\n",
    "    \n",
    "    # Prepare data for regression: Use new cases to predict mobility variables\n",
    "    for variable in color_map.keys():\n",
    "        train_data = cluster_data[cluster_data['week_start'] <= '2022-10-13']\n",
    "        \n",
    "        if len(train_data) > 0:\n",
    "            # Features and target variable\n",
    "            X = train_data[['new_cases']].values  # New cases as the feature\n",
    "            y = train_data[variable].values  # Mobility variable as the target\n",
    "            mask = ~np.isnan(y)\n",
    "            X = X[mask]\n",
    "            y = y[mask]\n",
    "            \n",
    "            if len(X) > 0: \n",
    "                model = LinearRegression()\n",
    "                model.fit(X, y)\n",
    "                \n",
    "                # Predict future values based on new cases (placeholder for future new cases)\n",
    "                future_new_cases = np.zeros((len(future_dates), 1))  \n",
    "                predicted_mobility = model.predict(future_new_cases)\n",
    "                \n",
    "                # Store predictions\n",
    "                if cluster not in predictions:\n",
    "                    predictions[cluster] = {}\n",
    "                predictions[cluster][variable] = predicted_mobility\n",
    "\n",
    "#line graphs for each cluster including predictions\n",
    "for cluster in np.unique(clusters):\n",
    "    countries_in_cluster = weekly_country_data[weekly_country_data['cluster'] == cluster]['country_region']\n",
    "    cluster_data = weekly_country_data[weekly_country_data['country_region'].isin(countries_in_cluster)]\n",
    "    \n",
    "    # Aggregate data by week for each mobility variable\n",
    "    weekly_data = cluster_data.groupby(['week_start']).agg(\n",
    "        retail=('retail', 'mean'),\n",
    "        grocery=('grocery', 'mean'),\n",
    "        parks=('parks', 'mean'),\n",
    "        transit=('transit', 'mean'),\n",
    "        workplaces=('workplaces', 'mean'),\n",
    "        residential=('residential', 'mean'),).reset_index()\n",
    "    \n",
    "    # Create the line graph\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add actual mobility variable traces\n",
    "    for variable, color in color_map.items():\n",
    "        fig.add_trace(go.Scatter(x=weekly_data['week_start'],y=weekly_data[variable],mode='lines+markers',name=f'Actual {variable.title()}',line=dict(color=color) ))\n",
    "        \n",
    "        # Add predicted mobility variable traces\n",
    "        if cluster in predictions and variable in predictions[cluster]:\n",
    "            fig.add_trace(go.Scatter(x=future_dates,y=predictions[cluster][variable],mode='lines',name=f'Predicted {variable.title()}',line=dict(color=color, dash='dash')  ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Weekly Mobility Variables for Cluster {cluster}',\n",
    "        xaxis_title='Week Beginning Date',\n",
    "        yaxis_title='Avg Percentage Change',\n",
    "        height=600)\n",
    "\n",
    "    # Display the figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: (Mathieu et al., 2020)\n",
    "#Dataset link: https://ourworldindata.org/covid-stringency-index\n",
    "\n",
    "\n",
    "file_path = 'covid-containment-and-health-index.csv'\n",
    "df_containment = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Containment measure scatter plot\n",
    "\n",
    "\n",
    "# Ensure 'Day' is in datetime format\n",
    "df_containment['Day'] = pd.to_datetime(df_containment['Day'])\n",
    "\n",
    "# Define the key periods with their date ranges\n",
    "key_periods = {\n",
    "    \"WHO Declaration\": (\"2020-03-11\", \"2020-03-11\"),\n",
    "    \"First Lockdown\": (\"2020-03-01\", \"2020-05-31\"),\n",
    "    \"Second Wave and Lockdown\": (\"2020-12-01\", \"2021-02-28\"),\n",
    "    \"Peak of the Delta Variant\": (\"2021-06-01\", \"2021-08-31\"),\n",
    "    \"Omicron Surge\": (\"2021-12-01\", \"2022-02-28\")}\n",
    "\n",
    "#Unique color palette for each country\n",
    "unique_countries = df_containment['Code'].unique()\n",
    "colors = {country: f'#{random.randint(0, 0xFFFFFF):06x}' for country in unique_countries}\n",
    "\n",
    "# Loop through each key period to create separate plots\n",
    "for period_name, (start_date, end_date) in key_periods.items():\n",
    "    period_data = df_containment[(df_containment['Day'] >= start_date) & (df_containment['Day'] <= end_date)]\n",
    "    # Calculating the average containment health index for each country\n",
    "    period_avg = period_data.groupby(['Entity', 'Code'])['Containment health index (average)'].mean().reset_index()\n",
    "    period_avg = period_avg.dropna()\n",
    "\n",
    "    # Create a scatter plot for the specific period\n",
    "    plt.figure(figsize=(14, 8))  \n",
    "    scatter = plt.scatter(period_avg['Code'], period_avg['Containment health index (average)'], color=[colors[code] for code in period_avg['Code']], s=150) \n",
    "\n",
    "    # Annotate points with country codes, with adjustments to recognise which plot beolong to which country\n",
    "    for i, row in period_avg.iterrows():\n",
    "        plt.text(row['Code'], row['Containment health index (average)'] + 1, row['Code'], horizontalalignment='center', size='medium', color='black', fontweight='bold')  \n",
    "\n",
    "\n",
    "    plt.title(f'Average Containment Health Index for {period_name}', fontsize=16)\n",
    "    plt.ylabel('Average Containment Health Index', fontsize=14)\n",
    "    # Remove x-axis ticks and labels\n",
    "    plt.xticks([]) \n",
    "    plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print top 3 and bottom 3 countries for each indivudal period\n",
    "    top_3 = period_avg.nlargest(3, 'Containment health index (average)')\n",
    "    bottom_3 = period_avg.nsmallest(3, 'Containment health index (average)')\n",
    "    \n",
    "    print(f'\\nFor Period: {period_name}')\n",
    "    print('Top 3 Countries:')\n",
    "    for index, row in top_3.iterrows():\n",
    "        print(f\"{row['Code']}: {row['Containment health index (average)']:.2f}\")\n",
    "    \n",
    "    print('Bottom 3 Countries:')\n",
    "    for index, row in bottom_3.iterrows():\n",
    "        print(f\"{row['Code']}: {row['Containment health index (average)']:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084efc3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Individual country level analysis\n",
    "#Reference: (Wikipedia.org, 2020)\n",
    "\n",
    "# Ensure date columns are in datetime format\n",
    "df_copy1_extended['date'] = pd.to_datetime(df_copy1_extended['date'])\n",
    "\n",
    "\n",
    "#policy events for each country using the national covid 19 response page on wikpededia\n",
    "policy_events = {\n",
    "    \"United Kingdom\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-23\"), \"end_date\": pd.to_datetime(\"2020-05-13\"), \"event\": \"First Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-11-05\"), \"end_date\": pd.to_datetime(\"2020-12-02\"), \"event\": \"Second Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-01-04\"), \"end_date\": pd.to_datetime(\"2021-03-29\"), \"event\": \"Third Lockdown\"}\n",
    "    ],\n",
    "    \"Sweden\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-15\"), \"end_date\": pd.to_datetime(\"2020-06-30\"), \"event\": \"Voluntary Restrictions\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-01-01\"), \"end_date\": pd.to_datetime(\"2021-03-31\"), \"event\": \"Gradual Policy Tightening\"}\n",
    "    ],\n",
    "    \"Australia\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-23\"), \"end_date\": pd.to_datetime(\"2020-05-11\"), \"event\": \"First Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-07-08\"), \"end_date\": pd.to_datetime(\"2020-10-27\"), \"event\": \"Melbourne Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-06-26\"), \"end_date\": pd.to_datetime(\"2021-07-30\"), \"event\": \"Sydney Lockdown\"}\n",
    "    ],\n",
    "    \"South Korea\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-02-23\"), \"end_date\": pd.to_datetime(\"2020-05-05\"), \"event\": \"Intensive Testing and Tracing\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-11-24\"), \"end_date\": pd.to_datetime(\"2021-01-31\"), \"event\": \"Limited Restrictions Reintroduced\"}\n",
    "    ],\n",
    "    \"Mongolia\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-11-12\"), \"end_date\": pd.to_datetime(\"2020-12-01\"), \"event\": \"Nationwide Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-01-01\"), \"end_date\": pd.to_datetime(\"2021-03-01\"), \"event\": \"Partial Restrictions\"}\n",
    "    ],\n",
    "    \"Libya\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-25\"), \"end_date\": pd.to_datetime(\"2020-06-01\"), \"event\": \"Initial Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-10-01\"), \"end_date\": pd.to_datetime(\"2020-12-31\"), \"event\": \"Relaxation of Restrictions\"}\n",
    "    ],\n",
    "    \"Argentina\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-20\"), \"end_date\": pd.to_datetime(\"2020-05-10\"), \"event\": \"Strict Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-10-01\"), \"end_date\": pd.to_datetime(\"2020-12-31\"), \"event\": \"Gradual Reopening\"}\n",
    "    ],\n",
    "    \"Bangladesh\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-26\"), \"end_date\": pd.to_datetime(\"2020-05-30\"), \"event\": \"Nationwide Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-04-01\"), \"end_date\": pd.to_datetime(\"2021-06-01\"), \"event\": \"Second Wave Lockdowns\"}\n",
    "    ],\n",
    "    \"Italy\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-09\"), \"end_date\": pd.to_datetime(\"2020-05-18\"), \"event\": \"National Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-10-25\"), \"end_date\": pd.to_datetime(\"2021-01-15\"), \"event\": \"New Restrictions\"}\n",
    "    ],\n",
    "    \"Mali\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-24\"), \"end_date\": pd.to_datetime(\"2020-06-01\"), \"event\": \"Initial Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-11-01\"), \"end_date\": pd.to_datetime(\"2020-12-31\"), \"event\": \"Relaxation of Restrictions\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Prepare weekly aggregated data\n",
    "df_copy1_extended['week_start'] = pd.to_datetime(df_copy1_extended['date']) - pd.to_timedelta(pd.to_datetime(df_copy1_extended['date']).dt.weekday, unit='D')\n",
    "weekly_country_data = df_copy1_extended.groupby(['country_region', 'week_start']).agg(\n",
    "    retail=('retail_and_recreation_percent_change_from_baseline', 'mean'),\n",
    "    workplaces=('workplaces_percent_change_from_baseline', 'mean'),\n",
    "    transit_stations=('transit_stations_percent_change_from_baseline', 'mean'),\n",
    "    residential=('residential_percent_change_from_baseline', 'mean'),\n",
    "    new_cases=('New_cases', 'sum'),\n",
    "    stay_at_home=('stay_at_home_requirements', 'mean')).reset_index()\n",
    "\n",
    "\n",
    "policy_colors = [\"LightSalmon\", \"LightGreen\", \"LightBlue\", \"LightCoral\", \"Khaki\"]\n",
    "\n",
    "# Function to plot time series data for a single country\n",
    "def plot_country_mobility(country_name):\n",
    "    country_data = weekly_country_data[weekly_country_data['country_region'] == country_name]\n",
    "\n",
    "    # Create a figure with mobility patterns and new cases\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add mobility patterns as individual traces (y-axis: left)\n",
    "    fig.add_trace(go.Scatter(x=country_data['week_start'], y=country_data['retail'],\n",
    "                   mode='lines', name='Retail & Recreation', yaxis='y1'))\n",
    "    fig.add_trace(go.Scatter(x=country_data['week_start'], y=country_data['workplaces'],\n",
    "                   mode='lines', name='Workplaces', yaxis='y1'))\n",
    "    fig.add_trace(go.Scatter(x=country_data['week_start'], y=country_data['transit_stations'],\n",
    "                   mode='lines', name='Transit Stations', yaxis='y1'))\n",
    "    fig.add_trace(go.Scatter(x=country_data['week_start'], y=country_data['residential'],\n",
    "                   mode='lines', name='Residential', yaxis='y1'))\n",
    "\n",
    "    # Add new cases (y-axis: right)\n",
    "    fig.add_trace(go.Scatter(x=country_data['week_start'], y=country_data['new_cases'],\n",
    "                   mode='lines', name='New Cases', yaxis='y2', line=dict(color='black')))\n",
    "\n",
    "    # Add shading for policy periods\n",
    "    for i, event in enumerate(policy_events.get(country_name, [])):\n",
    "        fig.add_shape(type=\"rect\",x0=event['start_date'], x1=event['end_date'],y0=0, y1=1,xref='x', yref='paper',fillcolor=policy_colors[i % len(policy_colors)], opacity=0.3, line_width=0,layer=\"below\")\n",
    "        fig.add_trace(go.Scatter(x=[None], y=[None],mode='lines',line=dict(color=policy_colors[i % len(policy_colors)], width=3),showlegend=True,name=event['event']))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f'Mobility Patterns and New Cases in {country_name}',\n",
    "        xaxis_title='Week Start',\n",
    "        yaxis=dict(\n",
    "            title='Mobility Change (%)',\n",
    "            side='left',\n",
    "            range=[-100, 100]  \n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            title='New Cases',\n",
    "            overlaying='y',\n",
    "            side='right'\n",
    "        ),\n",
    "        legend=dict(x=0.5, y=-0.3, orientation='h'),  \n",
    "        height=600,  \n",
    "        margin=dict(b=150) \n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    # Plot stay-at-home restrictions with policy shadings\n",
    "    fig_stay = go.Figure()\n",
    "    fig_stay.add_trace(go.Scatter(x=country_data['week_start'], y=country_data['stay_at_home'],\n",
    "                                  mode='lines', name='Stay-at-Home Requirements', line=dict(color='blue')))\n",
    "    \n",
    "    # Add shading for policy periods in the stay-at-home graph\n",
    "    for i, event in enumerate(policy_events.get(country_name, [])):\n",
    "        fig_stay.add_shape(type=\"rect\",x0=event['start_date'], x1=event['end_date'],y0=0, y1=3,xref='x', yref='y',fillcolor=policy_colors[i % len(policy_colors)], opacity=0.3, line_width=0,layer=\"below\")\n",
    "\n",
    "    fig_stay.update_layout(title_text=f'Stay-at-Home Requirements in {country_name}',xaxis_title='Week Start',\n",
    "        yaxis=dict(\n",
    "            title='Restriction Level',\n",
    "            tickmode='linear',  \n",
    "            dtick=1,            \n",
    "            range=[0, 3],       # Stay-at-home levels (0, 1, 2, 3)\n",
    "            side='left'\n",
    "        ),\n",
    "        height=350,  \n",
    "        margin=dict(t=30, b=50) )\n",
    "    \n",
    "    fig_stay.show()\n",
    "\n",
    "selected_countries = list(policy_events.keys())\n",
    "\n",
    "# Plot for each country in the selection\n",
    "for country in selected_countries:\n",
    "    plot_country_mobility(country)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictive modelling for the indivual countries using linear regression\n",
    "\n",
    "\n",
    "# Ensure date columns are in datetime format\n",
    "df_copy1_extended['date'] = pd.to_datetime(df_copy1_extended['date'])\n",
    "\n",
    "#Key policy events for each country\n",
    "policy_events = {\n",
    "    \"United Kingdom\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-23\"), \"end_date\": pd.to_datetime(\"2020-05-13\"), \"event\": \"First Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-11-05\"), \"end_date\": pd.to_datetime(\"2020-12-02\"), \"event\": \"Second Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-01-04\"), \"end_date\": pd.to_datetime(\"2021-03-29\"), \"event\": \"Third Lockdown\"}\n",
    "    ],\n",
    "    \"Sweden\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-15\"), \"end_date\": pd.to_datetime(\"2020-06-30\"), \"event\": \"Voluntary Restrictions\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-01-01\"), \"end_date\": pd.to_datetime(\"2021-03-31\"), \"event\": \"Gradual Policy Tightening\"}\n",
    "    ],\n",
    "    \"Australia\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-23\"), \"end_date\": pd.to_datetime(\"2020-05-11\"), \"event\": \"First Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-07-08\"), \"end_date\": pd.to_datetime(\"2020-10-27\"), \"event\": \"Melbourne Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-06-26\"), \"end_date\": pd.to_datetime(\"2021-07-30\"), \"event\": \"Sydney Lockdown\"}\n",
    "    ],\n",
    "    \"South Korea\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-02-23\"), \"end_date\": pd.to_datetime(\"2020-05-05\"), \"event\": \"Intensive Testing and Tracing\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-11-24\"), \"end_date\": pd.to_datetime(\"2021-01-31\"), \"event\": \"Limited Restrictions Reintroduced\"}\n",
    "    ],\n",
    "    \"Mongolia\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-11-12\"), \"end_date\": pd.to_datetime(\"2020-12-01\"), \"event\": \"Nationwide Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-01-01\"), \"end_date\": pd.to_datetime(\"2021-03-01\"), \"event\": \"Partial Restrictions\"}\n",
    "    ],\n",
    "    \"Libya\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-25\"), \"end_date\": pd.to_datetime(\"2020-06-01\"), \"event\": \"Initial Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-10-01\"), \"end_date\": pd.to_datetime(\"2020-12-31\"), \"event\": \"Relaxation of Restrictions\"}\n",
    "    ],\n",
    "    \"Argentina\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-20\"), \"end_date\": pd.to_datetime(\"2020-05-10\"), \"event\": \"Strict Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-10-01\"), \"end_date\": pd.to_datetime(\"2020-12-31\"), \"event\": \"Gradual Reopening\"}\n",
    "    ],\n",
    "    \"Bangladesh\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-26\"), \"end_date\": pd.to_datetime(\"2020-05-30\"), \"event\": \"Nationwide Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2021-04-01\"), \"end_date\": pd.to_datetime(\"2021-06-01\"), \"event\": \"Second Wave Lockdowns\"}\n",
    "    ],\n",
    "    \"Italy\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-09\"), \"end_date\": pd.to_datetime(\"2020-05-18\"), \"event\": \"National Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-10-25\"), \"end_date\": pd.to_datetime(\"2021-01-15\"), \"event\": \"New Restrictions\"}\n",
    "    ],\n",
    "    \"Mali\": [\n",
    "        {\"start_date\": pd.to_datetime(\"2020-03-24\"), \"end_date\": pd.to_datetime(\"2020-06-01\"), \"event\": \"Initial Lockdown\"},\n",
    "        {\"start_date\": pd.to_datetime(\"2020-11-01\"), \"end_date\": pd.to_datetime(\"2020-12-31\"), \"event\": \"Relaxation of Restrictions\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Prepare weekly aggregated data\n",
    "df_copy1_extended['week_start'] = pd.to_datetime(df_copy1_extended['date']) - pd.to_timedelta(pd.to_datetime(df_copy1_extended['date']).dt.weekday, unit='D')\n",
    "weekly_country_data = df_copy1_extended.groupby(['country_region', 'week_start']).agg(\n",
    "    retail=('retail_and_recreation_percent_change_from_baseline', 'mean'),\n",
    "    workplaces=('workplaces_percent_change_from_baseline', 'mean'),\n",
    "    transit_stations=('transit_stations_percent_change_from_baseline', 'mean'),\n",
    "    residential=('residential_percent_change_from_baseline', 'mean'),\n",
    "    new_cases=('New_cases', 'sum'),\n",
    "    stay_at_home=('stay_at_home_requirements', 'mean')).reset_index()\n",
    "\n",
    "\n",
    "selected_countries = list(policy_events.keys())\n",
    "\n",
    "#List to store predictions for each country\n",
    "predictions = []\n",
    "\n",
    "# Loop through each selected country and perform the analysis\n",
    "for country in selected_countries:\n",
    "    country_data = weekly_country_data[weekly_country_data['country_region'] == country]\n",
    "\n",
    "    if len(country_data) < 2:\n",
    "        print(f\"Not enough data for {country}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Filter data until the cutoff date\n",
    "    country_data = country_data[country_data['week_start'] <= '2022-10-13']\n",
    "\n",
    "    # Fill missing values in country data using forward fill\n",
    "    country_data[['new_cases', 'stay_at_home']] = country_data[['new_cases', 'stay_at_home']].fillna(method='ffill')\n",
    "\n",
    "    country_data[['retail', 'workplaces', 'transit_stations', 'residential']] = country_data[['retail', 'workplaces', 'transit_stations', 'residential']].fillna(method='ffill')\n",
    "    if country_data[['retail', 'workplaces', 'transit_stations', 'residential']].isnull().any().any():\n",
    "        print(f\"NaN values found in target variables for {country}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Split data into features (new_cases) and target (mobility variables)\n",
    "    X = country_data[['new_cases']]\n",
    "    y_retail = country_data['retail']\n",
    "    y_workplaces = country_data['workplaces']\n",
    "    y_transit = country_data['transit_stations']\n",
    "    y_residential = country_data['residential']\n",
    "\n",
    "    # Initialize models for each mobility variable\n",
    "    models = {'retail': LinearRegression(),'workplaces': LinearRegression(),'transit_stations': LinearRegression(),'residential': LinearRegression()}\n",
    "\n",
    "    # Train models for each target variable\n",
    "    for target, model in models.items():\n",
    "        model.fit(X, country_data[target])\n",
    "\n",
    "    # Predict future values until the last entry of new cases\n",
    "    last_date = country_data['week_start'].max()\n",
    "    future_dates = pd.date_range(start=last_date + pd.DateOffset(weeks=1), end='2024-08-04', freq='W-MON')\n",
    "    \n",
    "    last_new_cases = country_data['new_cases'].iloc[-1]\n",
    "    future_data = pd.DataFrame({'week_start': future_dates,'new_cases': last_new_cases,'country_region': country  })\n",
    "\n",
    "    # Predict each mobility variable for future data\n",
    "    future_predictions = {}\n",
    "    for target, model in models.items():\n",
    "        future_predictions[target] = model.predict(future_data[['new_cases']])\n",
    "\n",
    "    # Store predictions in a DataFrame\n",
    "    for target in models.keys():\n",
    "        future_data[target] = future_predictions[target]\n",
    "    \n",
    "    predictions.append(pd.concat([country_data[['week_start', 'country_region', 'retail', 'workplaces', 'transit_stations', 'residential']], future_data], ignore_index=True))\n",
    "\n",
    "# Combine all predictions into a single DataFrame\n",
    "predictions_df = pd.concat(predictions, ignore_index=True)\n",
    "\n",
    "# Plot the historical and predicted values for each selected country\n",
    "for country in selected_countries:\n",
    "    country_predictions = predictions_df[predictions_df['country_region'] == country]\n",
    "    \n",
    "    if country_predictions.empty:\n",
    "        continue  \n",
    "    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    #historical data\n",
    "    fig.add_trace(go.Scatter(x=country_predictions['week_start'], y=country_predictions['retail'], \n",
    "                             mode='lines', name='Historical Retail', line=dict(color='blue')))\n",
    "    fig.add_trace(go.Scatter(x=country_predictions['week_start'], y=country_predictions['workplaces'], \n",
    "                             mode='lines', name='Historical Workplaces', line=dict(color='green')))\n",
    "    fig.add_trace(go.Scatter(x=country_predictions['week_start'], y=country_predictions['transit_stations'], \n",
    "                             mode='lines', name='Historical Transit Stations', line=dict(color='orange')))\n",
    "    fig.add_trace(go.Scatter(x=country_predictions['week_start'], y=country_predictions['residential'], \n",
    "                             mode='lines', name='Historical Residential', line=dict(color='red')))\n",
    "\n",
    "    #predicted data \n",
    "    fig.add_trace(go.Scatter(x=country_predictions['week_start'], \n",
    "                             y=country_predictions['retail'].where(country_predictions['week_start'] > last_date), \n",
    "                             mode='lines', name='Predicted Retail', line=dict(color='blue', dash='dot')))\n",
    "    fig.add_trace(go.Scatter(x=country_predictions['week_start'], \n",
    "                             y=country_predictions['workplaces'].where(country_predictions['week_start'] > last_date), \n",
    "                             mode='lines', name='Predicted Workplaces', line=dict(color='green', dash='dot')))\n",
    "    fig.add_trace(go.Scatter(x=country_predictions['week_start'], \n",
    "                             y=country_predictions['transit_stations'].where(country_predictions['week_start'] > last_date), \n",
    "                             mode='lines', name='Predicted Transit Stations', line=dict(color='orange', dash='dot')))\n",
    "    fig.add_trace(go.Scatter(x=country_predictions['week_start'], \n",
    "                             y=country_predictions['residential'].where(country_predictions['week_start'] > last_date), \n",
    "                             mode='lines', name='Predicted Residential', line=dict(color='red', dash='dot')))\n",
    "\n",
    "    fig.update_layout(title=f'Mobility Predictions for {country}', \n",
    "                      xaxis_title='Week Start', \n",
    "                      yaxis_title='Mobility Change (%)')\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression predicting global mobility\n",
    "\n",
    "#Feature engineering\n",
    "df_filtered = df_copy1_extended[[\n",
    "    'continent', 'week_start', 'Cumulative_cases', 'stay_at_home_requirements',\n",
    "    'retail_and_recreation_percent_change_from_baseline',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline',\n",
    "    'transit_stations_percent_change_from_baseline',\n",
    "    'workplaces_percent_change_from_baseline',\n",
    "    'residential_percent_change_from_baseline']]\n",
    "\n",
    "# Rename columns for simplicity\n",
    "df_filtered.columns = ['continent', 'week_start', 'cumulative_cases', 'stay_home','retail', 'grocery', 'parks', 'transit', 'workplaces', 'residential']\n",
    "\n",
    "\n",
    "df_filtered.dropna(subset=['cumulative_cases', 'stay_home'], inplace=True)\n",
    "\n",
    "# Calculate weekly averages for each continent\n",
    "weekly_data = df_filtered.groupby(['continent', 'week_start']).agg(retail_avg=('retail', 'mean'),grocery_avg=('grocery', 'mean'),parks_avg=('parks', 'mean'),\n",
    "    transit_avg=('transit', 'mean'),\n",
    "    workplaces_avg=('workplaces', 'mean'),\n",
    "    residential_avg=('residential', 'mean'),\n",
    "    cumulative_cases_avg=('cumulative_cases', 'mean'),\n",
    "    stay_home_avg=('stay_home', 'mean')).reset_index()\n",
    "\n",
    "# Calculate global weekly averages\n",
    "global_weekly_data = df_filtered.groupby(['week_start']).agg(\n",
    "    retail_avg=('retail', 'mean'),\n",
    "    grocery_avg=('grocery', 'mean'),\n",
    "    parks_avg=('parks', 'mean'),\n",
    "    transit_avg=('transit', 'mean'),\n",
    "    workplaces_avg=('workplaces', 'mean'),\n",
    "    residential_avg=('residential', 'mean'),\n",
    "    cumulative_cases_avg=('cumulative_cases', 'mean'),\n",
    "    stay_home_avg=('stay_home', 'mean')).reset_index()\n",
    "\n",
    "\n",
    "global_weekly_data['week_start'] = pd.to_datetime(global_weekly_data['week_start'])\n",
    "\n",
    "#features and target variables\n",
    "features = ['cumulative_cases_avg', 'stay_home_avg']\n",
    "targets = ['retail_avg', 'grocery_avg', 'parks_avg', 'transit_avg', 'workplaces_avg', 'residential_avg']\n",
    "\n",
    "global_weekly_data[features] = global_weekly_data[features].fillna(method='ffill')\n",
    "\n",
    "# Split global data into training and prediction periods\n",
    "train_data = global_weekly_data[global_weekly_data['week_start'] <= '2022-10-13']\n",
    "test_data = global_weekly_data[global_weekly_data['week_start'] > '2022-10-13']\n",
    "\n",
    "\n",
    "test_data[features] = test_data[features].fillna(method='ffill')\n",
    "\n",
    "# Separate features and targets for training\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[targets]\n",
    "\n",
    "#Dictionary to hold models for each target\n",
    "models = {}\n",
    "\n",
    "# Training a linear regression model for each target variable\n",
    "for target in targets:\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train[target])\n",
    "    models[target] = model\n",
    "\n",
    "# Extend test data up to 2024-08-04\n",
    "future_dates = pd.date_range(start=test_data['week_start'].max(), end='2024-08-04', freq='W-MON')\n",
    "future_data = pd.DataFrame({'week_start': future_dates})\n",
    "\n",
    "# Add feature columns to future data (initialize with NaN)\n",
    "for feature in features:\n",
    "    future_data[feature] = np.nan\n",
    "\n",
    "# Forward fill test data and propagate into future data\n",
    "test_data[features] = test_data[features].fillna(method='ffill')\n",
    "future_data[features] = test_data[features].iloc[-1]  \n",
    "\n",
    "# Predict future values using trained models\n",
    "predictions_extended = {}\n",
    "\n",
    "for target in targets:\n",
    "    model = models[target]\n",
    "    extended_features = pd.concat([test_data[features], future_data], axis=0)\n",
    "    predictions_extended[target] = model.predict(extended_features[features])\n",
    "\n",
    "# Combine predictions with actual data\n",
    "predictions_df_extended = pd.concat([test_data[['week_start']], future_data], axis=0).reset_index(drop=True)\n",
    "for target in targets:\n",
    "    predictions_df_extended[f'{target}_predicted'] = predictions_extended[target]\n",
    "\n",
    "# Plot all mobility variables with cumulative cases on a secondary y-axis\n",
    "def plot_mobility_and_cases(global_data, predictions_df, targets, title):\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'cyan'] \n",
    "    for idx, target in enumerate(targets):\n",
    "        plt.plot(global_data['week_start'], global_data[target], label=f'Actual {target}', color=colors[idx], alpha=0.7)\n",
    "        plt.plot(predictions_df['week_start'], predictions_df[f'{target}_predicted'], label=f'Predicted {target}', color=colors[idx], linestyle='--')\n",
    "    \n",
    "    # Plot cumulative cases on the secondary y-axis\n",
    "    ax1 = plt.gca()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(global_data['week_start'], global_data['cumulative_cases_avg'], label='Cumulative Cases', color='black', linewidth=2, linestyle=':')\n",
    "    ax2.set_ylabel('Cumulative Cases', color='black')\n",
    "    ax1.set_xlabel('Week Start')\n",
    "    ax1.set_ylabel('Mobility Changes (%)')\n",
    "    plt.title(f'{title}: Mobility Variables and Cumulative Cases')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.xticks(rotation=45)\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#Global predictions for all mobility variables with cumulative cases\n",
    "plot_mobility_and_cases(global_weekly_data, predictions_df_extended, targets, 'Global Mobility and COVID-19 Cumulative Cases')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f5ed8",
   "metadata": {},
   "source": [
    "### Gender pay, economy and  Labour market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6f98f",
   "metadata": {},
   "source": [
    "### Labour market and economy dynamics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cf10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset link: https://ourworldindata.org/grapher/global-gdp-over-the-long-run?tab=table\n",
    "#Reference: (Our World in Data, 2017)\n",
    "# Load the dataset into a DataFrame called global gdp\n",
    "globalgdp = pd.read_csv('global-gdp-over-the-long-run.csv')\n",
    "globalgdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line graph of GDP over time\n",
    "\n",
    "# Filter the DataFrame for the years 2017 to 2022\n",
    "gdp_filtered = globalgdp[(globalgdp['Year'] >= 2017) & (globalgdp['Year'] <= 2022)]\n",
    "\n",
    "# Plot the line graph for GDP over the specified years\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(gdp_filtered['Year'], gdp_filtered['GDP'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Global GDP from 2017 to 2022')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('GDP (in Trillions)')\n",
    "plt.xticks(gdp_filtered['Year']) \n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET LINK: https://ourworldindata.org/grapher/real-gdp-growth\n",
    "#Refrence: (Our World in Data, 2020)\n",
    "\n",
    "# Load the dataset into a DataFrame called globalgdpgrowth\n",
    "globalgdpgrowth = pd.read_csv('real-gdp-growth.csv')\n",
    "globalgdpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Country name mapping\n",
    "\n",
    "corrections = {\n",
    "    'Antigua and Barbuda': 'Antigua and Barbuda',\n",
    "    'San Marino': 'San Marino',\n",
    "    'Eswatini': 'eSwatini',\n",
    "    'Hong Kong': 'Hong Kong',\n",
    "    'Aruba': 'Aruba',\n",
    "    'Barbados': 'Barbados',\n",
    "    'Marshall Islands': 'Marshall Islands',\n",
    "    'Saint Kitts and Nevis': 'Saint Kitts and Nevis',\n",
    "    'Korea': 'South Korea',\n",
    "    'Malta': 'Malta',\n",
    "    'Saint Vincent and the Grenadines': 'Saint Vincent and the Grenadines',\n",
    "    'Saint Lucia': 'Saint Lucia',\n",
    "    'Dominica': 'Dominica',\n",
    "    'Bahrain': 'Bahrain',\n",
    "    'East Timor': 'Timor-Leste',\n",
    "    'United States': 'United States of America',\n",
    "    'Samoa': 'Samoa',\n",
    "    'Palau': 'Palau',\n",
    "    'Sao Tome and Principe': 'Sao Tome and Principe',\n",
    "    'Bosnia and Herzegovina': 'Bosnia and Herz.',\n",
    "    'Dominican Republic': 'Dominican Rep.',\n",
    "    'Macao': 'Macao',\n",
    "    'Mauritius': 'Mauritius',\n",
    "    'Democratic Republic of Congo': 'Dem. Rep. Congo',\n",
    "    'Seychelles': 'Seychelles',\n",
    "    'Equatorial Guinea': 'Eq. Guinea',\n",
    "    'Grenada': 'Grenada',\n",
    "    'Micronesia (country)': 'Micronesia (country)',\n",
    "    'Cote d\\'Ivoire': \"Côte d'Ivoire\",\n",
    "    'Cape Verde': 'Cape Verde',\n",
    "    'Tuvalu': 'Tuvalu',\n",
    "    'Maldives': 'Maldives',\n",
    "    'Kiribati': 'Kiribati',\n",
    "    'Singapore': 'Singapore',\n",
    "    'Central African Republic': 'Central African Rep.',\n",
    "    'Solomon Islands' : 'Solomon Is.',\n",
    "    'Nauru': 'Nauru',\n",
    "    'Tonga': 'Tonga',\n",
    "    'Comoros': 'Comoros',\n",
    "    'South Sudan': 'S. Sudan',\n",
    "    'Andorra': 'Andorra'}\n",
    "\n",
    "# Apply corrections to the GDP dataset\n",
    "globalgdpgrowth['Entity'] = globalgdpgrowth['Entity'].replace(corrections)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ed88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global GDP growth rate\n",
    "\n",
    "# Filter data for the years 2015 to 2023\n",
    "filtered_data = globalgdpgrowth[(globalgdpgrowth['Year'] >= 2017) & (globalgdpgrowth['Year'] <= 2023)]\n",
    "\n",
    "#Dictionary to hold results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each year to find the highest and lowest growth countries\n",
    "for year in range(2017, 2024):\n",
    "    yearly_data = filtered_data[filtered_data['Year'] == year]\n",
    "    \n",
    "    # Finding the countries with the highest and lowest growth\n",
    "    highest_growth = yearly_data.nlargest(5, 'Gross domestic product, constant prices - Percent change - Observations')\n",
    "    lowest_growth = yearly_data.nsmallest(5, 'Gross domestic product, constant prices - Percent change - Observations')\n",
    "    \n",
    "    # Store results in the dictionary\n",
    "    results[year] = {\n",
    "        'highest_growth': highest_growth[['Entity', 'Gross domestic product, constant prices - Percent change - Observations']],\n",
    "        'lowest_growth': lowest_growth[['Entity', 'Gross domestic product, constant prices - Percent change - Observations']]}\n",
    "\n",
    "    # Plotting the results for each year\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot highest growth\n",
    "    ax.bar(highest_growth['Entity'], highest_growth['Gross domestic product, constant prices - Percent change - Observations'], label='Highest Growth', color='green')\n",
    "    \n",
    "    # Plot lowest growth\n",
    "    ax.bar(lowest_growth['Entity'], lowest_growth['Gross domestic product, constant prices - Percent change - Observations'], label='Lowest Growth', color='red')\n",
    "    ax.set_title(f'GDP Growth in {year}')\n",
    "    ax.set_ylabel('GDP Growth (%)')\n",
    "    ax.set_xlabel('Country')\n",
    "    ax.axhline(0, color='black', lw=0.8)  # Adding a line at y=0 for reference\n",
    "    ax.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Results for each year\n",
    "for year, data in results.items():\n",
    "    print(f\"Year: {year}\")\n",
    "    print(\"Highest Growth Countries:\")\n",
    "    print(data['highest_growth'])\n",
    "    print(\"\\nLowest Growth Countries:\")\n",
    "    print(data['lowest_growth'])\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Selected countries GDP growth rate\n",
    "\n",
    "# Filter data for the years 2017 to 2023 and the selected countries\n",
    "selected_countries = ['United Kingdom', 'Sweden', 'Australia', 'South Korea', 'Mongolia', 'Libya', 'Argentina', 'Bangladesh', 'Italy', 'Mali']\n",
    "filtered_data = globalgdpgrowth[(globalgdpgrowth['Year'] >= 2017) & (globalgdpgrowth['Year'] <= 2023)]\n",
    "filtered_data = filtered_data[filtered_data['Entity'].isin(selected_countries)]\n",
    "\n",
    "#List for results\n",
    "results = {}\n",
    "\n",
    "# Iterate over each year to store GDP growth rates of the selected countries\n",
    "for year in range(2017, 2024):\n",
    "    yearly_data = filtered_data[filtered_data['Year'] == year]\n",
    "    \n",
    "    # Store results in the dictionary\n",
    "    results[year] = yearly_data[['Entity', 'Gross domestic product, constant prices - Percent change - Observations']]\n",
    "\n",
    "    # Plotting the results for each year\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot GDP growth rates for the selected countries\n",
    "    ax.bar(yearly_data['Entity'], yearly_data['Gross domestic product, constant prices - Percent change - Observations'], color='skyblue')\n",
    "    ax.set_title(f'GDP Growth in {year} (Selected Countries)')\n",
    "    ax.set_ylabel('GDP Growth (%)')\n",
    "    ax.set_xlabel('Country')\n",
    "    ax.axhline(0, color='black', lw=0.8) \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display results for each year\n",
    "for year, data in results.items():\n",
    "    print(f\"Year: {year}\")\n",
    "    print(data)\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3749fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDP for g20 vs LDC \n",
    "\n",
    "#G20 and LDC countries\n",
    "g20_countries = ['Argentina', 'Australia', 'India', 'Saudi Arabia', 'South Africa', 'United Kingdom', 'United States of America']\n",
    "ldc_countries = ['Afghanistan', 'Haiti', 'Chad', 'Bangladesh', 'Yemen', 'Solomon Is.', 'Burkina Faso']\n",
    "\n",
    "# Filter data for the selected years and countries\n",
    "filtered_data = globalgdpgrowth[((globalgdpgrowth['Entity'].isin(g20_countries)) | (globalgdpgrowth['Entity'].isin(ldc_countries))) &(globalgdpgrowth['Year'] >= 2017) & (globalgdpgrowth['Year'] <= 2029)]\n",
    "\n",
    "# Calculating the average GDP growth for G20 and LDC countries per year\n",
    "g20_data = filtered_data[filtered_data['Entity'].isin(g20_countries)]\n",
    "ldc_data = filtered_data[filtered_data['Entity'].isin(ldc_countries)]\n",
    "\n",
    "# Aggregate the data for each year\n",
    "g20_avg_growth = g20_data.groupby('Year')['Gross domestic product, constant prices - Percent change - Observations'].mean()\n",
    "ldc_avg_growth = ldc_data.groupby('Year')['Gross domestic product, constant prices - Percent change - Observations'].mean()\n",
    "\n",
    "\n",
    "g20_forecast = g20_data.groupby('Year')['Gross domestic product, constant prices - Percent change - Forecasts'].mean()\n",
    "ldc_forecast = ldc_data.groupby('Year')['Gross domestic product, constant prices - Percent change - Forecasts'].mean()\n",
    "\n",
    "#Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "#Actual GDP growth line for G20 countries\n",
    "fig.add_trace(go.Scatter(x=g20_avg_growth.index,y=g20_avg_growth,mode='lines+markers',name='G20 (Actual)',line=dict(color='blue', width=2),marker=dict(size=6)))\n",
    "\n",
    "#forecast GDP growth line for G20 countries\n",
    "fig.add_trace(go.Scatter(x=g20_forecast.index,y=g20_forecast,mode='lines',name='G20 (Forecast)',line=dict(color='blue', dash='dash', width=2)))\n",
    "\n",
    "#Actual GDP growth line for LDC countries\n",
    "fig.add_trace(go.Scatter(x=ldc_avg_growth.index,y=ldc_avg_growth,mode='lines+markers',name='LDC (Actual)',line=dict(color='red', width=2),marker=dict(size=6)))\n",
    "\n",
    "#Forecast GDP growth line for LDC countries\n",
    "fig.add_trace(go.Scatter(x=ldc_forecast.index,y=ldc_forecast,mode='lines',name='LDC (Forecast)',line=dict(color='red', dash='dash', width=2)))\n",
    "\n",
    "\n",
    "fig.update_layout(title='Average GDP Growth (Constant Prices) for G20 and LDC Countries (2017-2029)',xaxis_title='Year',yaxis_title='GDP Growth (%)',legend_title='Group',template='plotly',hovermode='x unified')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b669ed",
   "metadata": {},
   "source": [
    "##### Unemployment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab49cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Link: https://ourworldindata.org/grapher/unemployment-rate-imf?tab=table&time=1982..latest\n",
    "#Reference: (Our World in Data, 2023)\n",
    "\n",
    "# Load the unemployment data into a DataFrame callled unemployment_data\n",
    "unemployment_data = pd.read_csv('unemployment-rate-imf.csv')\n",
    "\n",
    "unemployment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3092df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping country names accordingly to shapefile\n",
    "country_name_mapping = {\n",
    "    'United States': 'United States of America',\n",
    "    'Dominican Republic': 'Dominican Rep.',\n",
    "    'Bosnia and Herzegovina': 'Bosnia and Herz.',\n",
    "    'Korea': 'South Korea', \n",
    "    'Côte d\\'Ivoire': \"Côte d'Ivoire\",\n",
    "    'United States': 'United States of America',\n",
    "    'Curacao': 'Curaçao',\n",
    "    'Central African Rep.': 'Central African Republic',\n",
    "    'South Sudan': 'S. Sudan',\n",
    "    'North Korea': 'N. Korea',\n",
    "    'S. Korea': 'South Korea', \n",
    "    'Equatorial Guinea': 'Eq. Guinea',}\n",
    "\n",
    "#Appply mappings\n",
    "unemployment_data['Entity'] = unemployment_data['Entity'].replace(country_name_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8870f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global unemployment over time line graph \n",
    "\n",
    "# Filter for relevant columns and years for the line graph (2019-2023)\n",
    "unemployment_line_data = unemployment_data[['Entity', 'Year', 'Unemployment rate - Percent of total labor force - Observations']]\n",
    "unemployment_line_data = unemployment_line_data[(unemployment_line_data['Year'] >= 2019) & (unemployment_line_data['Year'] <= 2023)]\n",
    "\n",
    "# Renaming columns convention\n",
    "unemployment_line_data.rename(columns={'Entity': 'country_region','Unemployment rate - Percent of total labor force - Observations': 'unemployment_rate'}, inplace=True)\n",
    "\n",
    "# Calculating the global average unemployment rate for each year\n",
    "global_unemployment = unemployment_line_data.groupby('Year')['unemployment_rate'].mean()\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the global unemployment rate line\n",
    "fig.add_trace(go.Scatter(x=global_unemployment.index,y=global_unemployment,mode='lines+markers',name='Global Unemployment Rate',line=dict(color='firebrick', width=2),marker=dict(size=6)))\n",
    "\n",
    "fig.update_layout(title='Global Unemployment Rate (Percent of Total Labor Force) from 2019 to 2023',xaxis_title='Year',yaxis_title='Unemployment Rate (%)',template='plotly',hovermode='x unified')\n",
    "\n",
    "# Show the graph\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becccd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global unemployment including forecasts\n",
    "\n",
    "# Filter for relevant columns and years for the line graph (2019-2029)\n",
    "unemployment_line_data = unemployment_data[['Entity', 'Year', 'Unemployment rate - Percent of total labor force - Observations', 'Unemployment rate - Percent of total labor force - Forecasts']]\n",
    "unemployment_line_data = unemployment_line_data[(unemployment_line_data['Year'] >= 2019) & (unemployment_line_data['Year'] <= 2029)]\n",
    "\n",
    "# Renaming columns for easier access\n",
    "unemployment_line_data.rename(columns={'Entity': 'country_region','Unemployment rate - Percent of total labor force - Observations': 'unemployment_rate','Unemployment rate - Percent of total labor force - Forecasts': 'forecast_unemployment_rate'}, inplace=True)\n",
    "\n",
    "# Separate actual and forecast data\n",
    "global_unemployment_actual = unemployment_line_data[unemployment_line_data['Year'] <= 2023].groupby('Year')['unemployment_rate'].mean()\n",
    "global_unemployment_forecast = unemployment_line_data[unemployment_line_data['Year'] >= 2024].groupby('Year')['forecast_unemployment_rate'].mean()\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plotting actual global unemployment rate\n",
    "fig.add_trace(go.Scatter(x=global_unemployment_actual.index,y=global_unemployment_actual,mode='lines+markers',name='Global Unemployment Rate (Actual)',line=dict(color='firebrick', width=2),marker=dict(size=6)))\n",
    "\n",
    "# Plotting forecasted global unemployment rate with a dotted line starting from 2024\n",
    "fig.add_trace(go.Scatter(x=global_unemployment_forecast.index,y=global_unemployment_forecast,mode='lines',name='Global Unemployment Rate (Forecast)',line=dict(color='firebrick', width=2, dash='dash')))\n",
    "\n",
    "#Layout\n",
    "fig.update_layout(title='Global Unemployment Rate (Percent of Total Labor Force) from 2019 to 2029',xaxis_title='Year',yaxis_title='Unemployment Rate (%)',template='plotly',hovermode='x unified')\n",
    "\n",
    "# Show the graph\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d629a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the unemployment data\n",
    "unemployment_data = pd.read_csv('unemployment-rate-imf.csv')\n",
    "\n",
    "# Select the relevant columns\n",
    "unemployment_data = unemployment_data[['Year', 'Unemployment rate - Percent of total labor force - Observations', 'Unemployment rate - Percent of total labor force - Forecasts']]\n",
    "\n",
    "# Filter for the relevant years (2015 to 2029)\n",
    "unemployment_data = unemployment_data[(unemployment_data['Year'] >= 2015) & (unemployment_data['Year'] <= 2029)]\n",
    "\n",
    "# Group by year and calculate the mean unemployment rate\n",
    "global_unemployment = unemployment_data.groupby('Year').agg(global_unemployment_observed=('Unemployment rate - Percent of total labor force - Observations', 'mean'),global_unemployment_forecasted=('Unemployment rate - Percent of total labor force - Forecasts', 'mean')).reset_index()\n",
    "\n",
    "# Display the global unemployment DataFrame\n",
    "global_unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdbc0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging\n",
    "\n",
    "#Filter the globalgdpgrowth DataFrame for GDP data from 2019 to 2022\n",
    "gdp_data = globalgdpgrowth[(globalgdpgrowth['Year'] >= 2019) & (globalgdpgrowth['Year'] <= 2022)][['Entity', 'Year', 'Gross domestic product, constant prices - Percent change - Observations']]\n",
    "\n",
    "# Rename columns for easier merging\n",
    "gdp_data.rename(columns={'Entity': 'country_region', 'Year': 'year', 'Gross domestic product, constant prices - Percent change - Observations': 'gdp'}, inplace=True)\n",
    "\n",
    "#Merge GDP data into dfforgdp (a copy of df_copy) without aggregating\n",
    "dfforgdp = df_copy.copy()  \n",
    "dfforgdp = dfforgdp.merge(gdp_data, on=['country_region', 'year'], how='left')\n",
    "\n",
    "#Fill the GDP values for 2019-2022 in the new dataframe\n",
    "dfforgdp['gdp'] = dfforgdp.groupby('country_region')['gdp'].ffill()\n",
    "\n",
    "# Display the updated dfforgdp DataFrame\n",
    "dfforgdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the unemployment data\n",
    "unemployment_data = pd.read_csv('unemployment-rate-imf.csv')\n",
    "\n",
    "# Make a copy of the unemployment data for global unemployment\n",
    "global_unemploymentdata = unemployment_data.copy()\n",
    "\n",
    "# Select relevant columns\n",
    "global_unemploymentdata = global_unemploymentdata[['Entity', 'Year', 'Unemployment rate - Percent of total labor force - Observations', 'Unemployment rate - Percent of total labor force - Forecasts']]\n",
    "\n",
    "# Filter for the relevant years (2015 to 2029)\n",
    "global_unemploymentdata = global_unemploymentdata[(global_unemploymentdata['Year'] >= 2015) & (global_unemploymentdata['Year'] <= 2029)]\n",
    "\n",
    "# Separate observed and forecasted data\n",
    "observed_data = global_unemploymentdata[global_unemploymentdata['Year'] <= 2022].copy()\n",
    "forecasted_data = global_unemploymentdata[global_unemploymentdata['Year'] > 2022].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948332ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging\n",
    "\n",
    "#Filter the unemployment_data DataFrame for the relevant years (2019 to 2023)\n",
    "unemployment_data_filtered = unemployment_data[(unemployment_data['Year'] >= 2019) & (unemployment_data['Year'] <= 2023)]\n",
    "\n",
    "#Rename columns for easier merging\n",
    "unemployment_data_filtered.rename(columns={'Entity': 'country_region',  'Year': 'year', 'Unemployment rate - Percent of total labor force - Observations': 'unemployment_rate_observed'}, inplace=True)\n",
    "\n",
    "#Merge unemployment data into dfforgdp\n",
    "dfforgdp = dfforgdp.merge(unemployment_data_filtered, on=['country_region', 'year'], how='left')\n",
    "\n",
    "#Fill the unemployment values for the entire year since this data is yearly based\n",
    "dfforgdp['unemployment_rate_observed'] = dfforgdp.groupby('country_region')['unemployment_rate_observed'].ffill()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix each year\n",
    "\n",
    "years = [2020, 2021, 2022]\n",
    "\n",
    "# Loop over each year to generate a correlation matrix\n",
    "for year in years:\n",
    "    year_data = dfforgdp[dfforgdp['date'].dt.year == year]\n",
    "    \n",
    "    # Select relevant columns for the correlation matrix\n",
    "    correlation_data = year_data[['retail_and_recreation_percent_change_from_baseline','grocery_and_pharmacy_percent_change_from_baseline','parks_percent_change_from_baseline','transit_stations_percent_change_from_baseline','workplaces_percent_change_from_baseline','residential_percent_change_from_baseline','gdp', 'unemployment_rate_observed']]\n",
    "    \n",
    "    # Calculate the correlation matrix \n",
    "    correlation_matrix = correlation_data.corr()\n",
    "\n",
    "    #Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "    plt.title(f'Correlation Matrix for {year}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf894f",
   "metadata": {},
   "source": [
    " ### Labour force participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset link : https://data.worldbank.org/indicator/SL.TLF.CACT.FE.ZS\n",
    "#reference: (World Bank Open Data, 2024)\n",
    "\n",
    "labourforceparticipation = pd.read_csv('1d26f351-6cf3-4ae4-8689-5bc11b1ad293_Data.csv')\n",
    "labourforceparticipation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bce76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe handling\n",
    "\n",
    "#Adjust the DataFrame to long format for better readability\n",
    "labourforce_long = labourforceparticipation.melt(id_vars=['Series Name', 'Series Code', 'Country Name', 'Country Code'], var_name='Year', value_name='Participation Rate')\n",
    "\n",
    "#Clean up the Year column to extract the year as an integer\n",
    "labourforce_long['Year'] = labourforce_long['Year'].str.extract(r'(\\d{4})')[0].astype(int)\n",
    "\n",
    "#Convert 'Participation Rate' to numeric, handling errors\n",
    "labourforce_long['Participation Rate'] = pd.to_numeric(labourforce_long['Participation Rate'], errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#country name mapping\n",
    "\n",
    "name_adjustments = {\n",
    "    'Middle East & North Africa': 'Middle East and North Africa',\n",
    "    'Turkiye': 'Turkey',\n",
    "    'Russian Federation': 'Russia',\n",
    "    'United States': 'United States of America',\n",
    "    'Cote d\\'Ivoire': \"Côte d'Ivoire\",\n",
    "    'Brunei Darussalam': 'Brunei',\n",
    "    'Korea, Rep.': 'South Korea',\n",
    "    'Korea, Dem. People\\'s Rep.': 'North Korea',\n",
    "    'Viet Nam': 'Vietnam',\n",
    "    'Bahamas, The': 'Bahamas',\n",
    "    'World': 'Global',\n",
    "    'Isle of Man': 'Isle of Man',\n",
    "    'Samoa': 'Samoa',\n",
    "    'West Bank and Gaza': 'Palestine',\n",
    "    'Turks and Caicos Islands': 'Turks and Caicos',\n",
    "    'Gambia, The': 'Gambia',\n",
    "    'Dominican Republic': 'Dominican Republic',\n",
    "    'Macao SAR, China': 'Macau',\n",
    "    'South Sudan': 'South Sudan',\n",
    "    'Egypt, Arab Rep.': 'Egypt',\n",
    "    'Sao Tome and Principe': 'Sao Tome and Principe',\n",
    "    'Solomon Islands': 'Solomon Is.',\n",
    "    'Dominican Rep.': 'Dominican Republic',\n",
    "    'Central African Rep.': 'Central African Republic',\n",
    "    'Bosnia and Herz.': 'Bosnia and Herzegovina',\n",
    "    'W. Sahara': 'Western Sahara',\n",
    "    'Fr. S. Antarctic Lands': 'French Southern Territories',\n",
    "    'Laos': 'Lao PDR',\n",
    "    'eSwatini': 'Eswatini',\n",
    "    'Iran': 'Iran, Islamic Republic of',\n",
    "    'Taiwan': 'Taiwan, Province of China',\n",
    "    'S. Sudan': 'South Sudan',\n",
    "    'Dem. Rep. Congo': 'Congo, Dem. Rep.',\n",
    "    'Eq. Guinea': 'Equatorial Guinea',\n",
    "    'N. Cyprus': 'Northern Cyprus',\n",
    "    'Turkey': 'Turkiye',\n",
    "    'Vietnam': 'Viet Nam',\n",
    "    'Venezuela': 'Venezuela, RB',\n",
    "    'Egypt': 'Egypt, Arab Rep.',\n",
    "    'Seychelles': 'Seychelles',\n",
    "    'Marshall Islands': 'Marshall Islands',\n",
    "    'Central Europe and the Baltics': 'Central Europe and the Baltic States',\n",
    "    'Bahamas': 'Bahamas',\n",
    "    'Kiribati': 'Kiribati',\n",
    "    'Micronesia, Fed. Sts.': 'Micronesia',\n",
    "    'United States of America': 'United States',\n",
    "    'Saint Kitts and Nevis': 'St. Kitts and Nevis',\n",
    "    'Saint Vincent and the Grenadines': 'St. Vincent and the Grenadines',\n",
    "    'Sao Tome and Principe': 'Sao Tome and Principe',\n",
    "    'Saint Lucia': 'St. Lucia',\n",
    "    'Turks and Caicos Islands': 'Turks and Caicos',\n",
    "    'Grenada': 'Grenada',\n",
    "    'Macao SAR, China': 'Macao',\n",
    "    'Timor-Leste': 'Timor Leste',\n",
    "    'Sierra Leone': 'Sierra Leone',\n",
    "    'Kyrgyz Republic': 'Kyrgyzstan',\n",
    "    'Congo, Dem. Rep.': 'Democratic Republic of the Congo',\n",
    "    'Congo, Rep.': 'Republic of the Congo',\n",
    "    'North Korea': \"Korea, Dem. People's Rep.\",\n",
    "    'Yemen, Rep.': 'Yemen',\n",
    "    'Iran, Islamic Rep.': 'Iran',\n",
    "    'Kyrgyzstan': 'Kyrgyz Republic',\n",
    "    'French Polynesia': 'French Polynesia',\n",
    "    'Seychelles': 'Seychelles',\n",
    "    'Hong Kong SAR, China': 'Hong Kong',\n",
    "    'Dominica': 'Dominica',\n",
    "    'Comoros': 'Comoros',\n",
    "    'Malawi': 'Malawi',\n",
    "    'Tonga': 'Tonga',\n",
    "    'Tuvalu': 'Tuvalu',\n",
    "    'Maldives': 'Maldives',\n",
    "    'American Samoa': 'American Samoa',\n",
    "    'Northern Mariana Islands': 'Northern Mariana Islands',\n",
    "    'Gibraltar': 'Gibraltar',\n",
    "    'Palau': 'Palau',\n",
    "    'Micronesia, Fed. Sts.': 'Micronesia',\n",
    "    'Guam': 'Guam',\n",
    "    'Antigua and Barbuda': 'Antigua and Barbuda',\n",
    "    'Curacao': 'Curaçao',\n",
    "    'Cabo Verde': 'Cabo Verde',\n",
    "    'Solomon Islands': 'Solomon Is.',\n",
    "    'Malta': 'Malta',\n",
    "    'Saint Martin (French part)': 'Saint Martin',\n",
    "    'Macao SAR, China': 'Macau',\n",
    "    'Brunei Darussalam': 'Brunei',\n",
    "    'Dominican Republic': 'Dominican Rep.',\n",
    "    'Central African Republic': 'Central African Rep.',\n",
    "    'Bosnia and Herzegovina': 'Bosnia and Herz.',\n",
    "    'Republic of the Congo': 'Congo',\n",
    "    'South Sudan': 'S. Sudan',\n",
    "    'Equatorial Guinea': 'Eq. Guinea',\n",
    "    'Democratic Republic of the Congo': 'Dem. Rep. Congo',\n",
    "    'Lao PDR': 'Laos',\n",
    "    'Northern Cyprus': 'N. Cyprus',\n",
    "    'Venezuela, RB': 'Venezuela',\n",
    "    'Eswatini': 'eSwatini', \n",
    "    'Iran, Islamic Republic of': 'Iran',\n",
    "    \"Korea, Dem. People's Rep.\": 'North Korea',\n",
    "    'Kyrgyz Republic': 'Kyrgyzstan',\n",
    "    'United States': 'United States of America',\n",
    "    'Egypt, Arab Rep.': 'Egypt',\n",
    "    'Viet Nam': 'Vietnam',\n",
    "    'Turkiye': 'Turkey',\n",
    "    'Slovak Republic': 'Slovakia',\n",
    "    'Northern Cyprus': 'N. Cyprus',\n",
    "    'Falkland Islands': 'Falkland Is.',\n",
    "    'Syrian Arab Republic': 'Syria',\n",
    "    'Taiwan, Province of China': 'Taiwan',\n",
    "    'Western Sahara': 'W. Sahara',\n",
    "    'French Southern Territories': 'Fr. S. Antarctic Lands'}\n",
    "\n",
    "#Apply mappings\n",
    "labourforce_long['Country Name'] = labourforce_long['Country Name'].replace(name_adjustments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff81b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labour force participation line graph - regional\n",
    "\n",
    "#Specified groups already in the dataset\n",
    "\n",
    "filtered_data = labourforce_long[\n",
    "    labourforce_long['Country Name'].isin(['Europe & Central Asia','Sub-Saharan Africa','Africa Western and Central','Africa Eastern and Southern','East Asia & Pacific','Middle East & North Africa','Latin America & Caribbean','North America','South Asia','Central Europe and the Baltics']) & labourforce_long['Year'].between(2014, 2023)]\n",
    "\n",
    "# Create the line graph\n",
    "fig = px.line(filtered_data,x='Year',y='Participation Rate',color='Country Name',title='Labor Force Participation Rates (2014-2023)',labels={'Participation Rate': 'Labor Force Participation Rate (%)'})\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "#Show values for every region\n",
    "print(\"Labor Force Participation Rates (2014-2023):\")\n",
    "for country in filtered_data['Country Name'].unique():\n",
    "    country_data = filtered_data[filtered_data['Country Name'] == country]\n",
    "    print(f\"\\n{country}:\")\n",
    "    for index, row in country_data.iterrows():\n",
    "        print(f\"  Year {int(row['Year'])}: {row['Participation Rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80573d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Line graph of labour force participation -economic classification\n",
    "\n",
    "#Specified groups \n",
    "filtered_data = labourforce_long[labourforce_long['Country Name'].isin(['European Union', 'Least developed countries: UN classification', 'Heavily indebted poor countries (HIPC)', 'Fragile and conflict affected situations', 'High income', 'Euro area','Low income','Middle income','Lower middle income','Upper middle income','OECD members']) & labourforce_long['Year'].between(2010, 2023)]\n",
    "\n",
    "#line graph\n",
    "fig = px.line(filtered_data,x='Year',y='Participation Rate',color='Country Name',title='Labor Force Participation Rates (2014-2023)',labels={'Participation Rate': 'Labor Force Participation Rate (%)'})\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "#Value list\n",
    "print(\"Labor Force Participation Rates (2014-2023):\")\n",
    "for country in filtered_data['Country Name'].unique():\n",
    "    country_data = filtered_data[filtered_data['Country Name'] == country]\n",
    "    print(f\"\\n{country}:\")\n",
    "    for index, row in country_data.iterrows():\n",
    "        print(f\"  Year {int(row['Year'])}: {row['Participation Rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6707142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the participation rate into df_copy based on country and year\n",
    "df_copy = df_copy.merge(labourforce_long[['Country Name', 'Year', 'Participation Rate']],left_on=['country_region', 'year'], right_on=['Country Name', 'Year'],   how='left')\n",
    "\n",
    "df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548660d",
   "metadata": {},
   "source": [
    "### Gender pay gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8308b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Link: https://www.statista.com/statistics/1212140/global-gender-pay-gap/#:~:text=By%20comparison%2C%20the%20uncontrolled%20gender,every%20dollar%20earned%20by%20men.\n",
    "#reference: (Statista, 2024)\n",
    "\n",
    "#Obtain the excel file\n",
    "file_path = 'statistic_id1212140_global-gender-pay-gap-2015-2024 (1).xlsx'\n",
    "\n",
    "# Read the dataset from sheet 2 in the excel file\n",
    "gender_pay_gap_data = pd.read_excel(file_path, sheet_name=1)  \n",
    "gender_pay_gap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e990c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#line graph of gender pay gap over time\n",
    "\n",
    "#converting data into correct format\n",
    "cleaned_data = gender_pay_gap_data.iloc[4:14, [1, 2, 3]].reset_index(drop=True)  \n",
    "cleaned_data.columns = ['Year', 'Controlled', 'Uncontrolled']  \n",
    "\n",
    "cleaned_data['Year'] = cleaned_data['Year'].astype(int)\n",
    "cleaned_data['Controlled'] = cleaned_data['Controlled'].astype(float)\n",
    "cleaned_data['Uncontrolled'] = cleaned_data['Uncontrolled'].astype(float)\n",
    "\n",
    "# Create the line graph\n",
    "fig = px.line(cleaned_data,x='Year',y=['Controlled', 'Uncontrolled'],title='Global Gender Pay Gap (2015-2024)',labels={'value': 'Gender Pay Gap Ratio', 'variable': 'Type'},markers=True)\n",
    "fig.update_layout(yaxis_title='Gender Pay Gap Ratio',xaxis_title='Year',legend_title='Type of Pay Gap')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efe62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDP, Gender Pay gap an Labour force participation over the years\n",
    "\n",
    "# Make a copy of the unemployment data for global unemployment\n",
    "global_unemploymentdata = unemployment_data.copy()\n",
    "\n",
    "# Select relevant columns\n",
    "global_unemploymentdata = global_unemploymentdata[['Entity', 'Year', 'Unemployment rate - Percent of total labor force - Observations', 'Unemployment rate - Percent of total labor force - Forecasts']]\n",
    "\n",
    "global_unemploymentdata = global_unemploymentdata[(global_unemploymentdata['Year'] >= 2015) & (global_unemploymentdata['Year'] <= 2029)]\n",
    "\n",
    "# Separate observed and forecasted data\n",
    "observed_data = global_unemploymentdata[global_unemploymentdata['Year'] <= 2022].copy()\n",
    "forecasted_data = global_unemploymentdata[global_unemploymentdata['Year'] > 2022].copy()\n",
    "\n",
    "# Group by year and calculate the mean unemployment rate for observed data\n",
    "global_unemployment_observed = observed_data.groupby('Year').agg(global_unemployment_observed=('Unemployment rate - Percent of total labor force - Observations', 'mean')).reset_index()\n",
    "\n",
    "# For the forecasted data, only need the mean of the forecasts\n",
    "global_unemployment_forecasted = forecasted_data.groupby('Year').agg(global_unemployment_forecasted=('Unemployment rate - Percent of total labor force - Forecasts', 'mean')).reset_index()\n",
    "\n",
    "# Combine observed and forecasted data to display on the graph\n",
    "global_unemployment = pd.merge(global_unemployment_observed,global_unemployment_forecasted,on='Year',how='outer')\n",
    "\n",
    "#Global GDP data manually written \n",
    "global_gdp = {\n",
    "    'Year': [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022],\n",
    "    'GDP': [114471780000000, 118180714000000, 122635850000000, 127074744000000, \n",
    "            130661590000000, 126791940000000, 134821810000000, 139357755000000]}\n",
    "gdp_data = pd.DataFrame(global_gdp)\n",
    "\n",
    "# Convert GDP to trillions for easier readability\n",
    "gdp_data['GDP'] = gdp_data['GDP'] / 1e12  \n",
    "\n",
    "\n",
    "# Clean the gender pay gap dataset\n",
    "cleaned_gap_data = gender_pay_gap_data.iloc[4:14, [1, 2, 3]].reset_index(drop=True)  # Extracting rows with data\n",
    "cleaned_gap_data.columns = ['Year', 'Controlled', 'Uncontrolled']  # Renaming columns\n",
    "\n",
    "# Convert the Year to integer and the values to float\n",
    "cleaned_gap_data['Year'] = cleaned_gap_data['Year'].astype(int)\n",
    "cleaned_gap_data['Controlled'] = cleaned_gap_data['Controlled'].astype(float)\n",
    "cleaned_gap_data['Uncontrolled'] = cleaned_gap_data['Uncontrolled'].astype(float)\n",
    "\n",
    "# Create a plot with three y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plotting global unemployment on the first y-axis\n",
    "ax1.plot(global_unemployment['Year'], global_unemployment['global_unemployment_observed'], marker='o', linestyle='-', color='b', label='Global Unemployment (Observed)')\n",
    "ax1.plot(global_unemployment['Year'], global_unemployment['global_unemployment_forecasted'], marker='x', linestyle='--', color='c', label='Global Unemployment (Forecasted)')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Unemployment Rate (%)', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "ax1.set_xticks(global_unemployment['Year'])\n",
    "ax1.grid()\n",
    "\n",
    "#Second y-axis for GDP\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(gdp_data['Year'], gdp_data['GDP'], marker='s', linestyle='-', color='r', label='Global GDP (Trillions)')\n",
    "ax2.set_ylabel('GDP (in Trillions)', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "#Third y-axis for Gender Pay Gap\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 60)) \n",
    "ax3.plot(cleaned_gap_data['Year'], cleaned_gap_data['Controlled'], marker='d', linestyle='-', color='m', label='Gender Pay Gap (Controlled)')\n",
    "ax3.plot(cleaned_gap_data['Year'], cleaned_gap_data['Uncontrolled'], marker='^', linestyle='--', color='y', label='Gender Pay Gap (Uncontrolled)')\n",
    "ax3.set_ylabel('Gender Pay Gap Ratio', color='m')\n",
    "ax3.tick_params(axis='y', labelcolor='m')\n",
    "\n",
    "\n",
    "plt.title('Global Unemployment, GDP, and Gender Pay Gap (2015-2029)')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "ax3.legend(loc='center right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ad252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Link: https://www.statista.com/statistics/1212189/workplace-gender-gap-worldwide-by-type/\n",
    "#Reference: (Statista, 2024)\n",
    "\n",
    "\n",
    "file_path = 'statistic_id1212189_workplace-gender-gap-worldwide-2024-by-type.xlsx'  \n",
    "\n",
    "# Load the dataset from the excel file\n",
    "gender_pay_gap_workplace = pd.read_excel(file_path,  sheet_name=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot to visualise gender pay gap by worktype\n",
    "\n",
    "\n",
    "# Extract relevant rows and columns\n",
    "cleaned_data = gender_pay_gap_workplace.iloc[4:10, [1, 2]].reset_index(drop=True)  \n",
    "#renaming columsn since the columsn from the raw dataset are not clear\n",
    "cleaned_data.columns = ['Category', 'Percentage']  \n",
    "\n",
    "# Convert the Percentage column to float\n",
    "cleaned_data['Percentage'] = cleaned_data['Percentage'].astype(float)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(cleaned_data['Category'], cleaned_data['Percentage'], color='skyblue')\n",
    "plt.title('Workplace Gender Gap Worldwide (2024) by Type')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Dataset Link: https://www.statista.com/statistics/244387/the-global-gender-gap-index/\n",
    "#Reference: (Statista, 2024)\n",
    "\n",
    "#Obtaining the excel file\n",
    "file_path = 'statistic_id244387_the-global-gender-gap-index-2024.xlsx'\n",
    "\n",
    "# Load the dataset from sheet 2 \n",
    "gender_pay_gap_countries = pd.read_excel(file_path, sheet_name=1) \n",
    "\n",
    "\n",
    "gender_pay_gap_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49260e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting the dataframe\n",
    "\n",
    "# Filter the DataFrame to start from row 4 and reset the index\n",
    "gender_pay_gap_countries = gender_pay_gap_countries.iloc[4:].reset_index(drop=True)\n",
    "\n",
    "# Drop the \"Unnamed: 0\" column\n",
    "gender_pay_gap_countries = gender_pay_gap_countries.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Rename columns\n",
    "gender_pay_gap_countries.columns = ['country_region', 'gender pay gap 2024']\n",
    "\n",
    "# Convert the gender pay gap column to float\n",
    "gender_pay_gap_countries['gender pay gap 2024'] = gender_pay_gap_countries['gender pay gap 2024'].astype(float)\n",
    "\n",
    "\n",
    "gender_pay_gap_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed625b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual plot \n",
    "\n",
    "# Find the 5 countries with the highest and lowest gender pay gaps\n",
    "top_countries = gender_pay_gap_countries.nlargest(10, 'gender pay gap 2024')\n",
    "bottom_countries = gender_pay_gap_countries.nsmallest(10, 'gender pay gap 2024')\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(gender_pay_gap_countries,x='country_region',y='gender pay gap 2024',title='Gender Pay Gap by Country (2024)',labels={'gender pay gap 2024': 'Gender Pay Gap (%)'},hover_name='country_region', color='gender pay gap 2024',  color_continuous_scale=px.colors.sequential.Plasma,  size_max=20  )\n",
    "\n",
    "\n",
    "fig.update_layout(yaxis=dict(title='Gender Pay Gap (%)', range=[0, 1], tickvals=[i/100 for i in range(0, 101, 10)], ticktext=[f'{i}%' for i in range(0, 101, 10)]),xaxis_title='Country',xaxis_tickangle=-45,showlegend=False)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# Print the top and bottom countries\n",
    "print(\"Top 5 Countries with the Highest Gender Pay Gap:\")\n",
    "print(top_countries)\n",
    "\n",
    "print(\"\\nBottom 5 Countries with the Least Gender Pay Gap:\")\n",
    "print(bottom_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe adjustment\n",
    "\n",
    "\n",
    "\n",
    "# Remove the first 4 rows and reset index\n",
    "gender_pay_gap_data = gender_pay_gap_data.iloc[4:].reset_index(drop=True)\n",
    "\n",
    "# Rename columns to differnet names\n",
    "gender_pay_gap_data.columns = ['Unnamed', 'Year', 'Controlled', 'Uncontrolled']\n",
    "\n",
    "# Converting year and pay gap values to appropriate data types\n",
    "gender_pay_gap_data['Year'] = gender_pay_gap_data['Year'].astype(int)\n",
    "gender_pay_gap_data['Controlled'] = gender_pay_gap_data['Controlled'].astype(float)\n",
    "gender_pay_gap_data['Uncontrolled'] = gender_pay_gap_data['Uncontrolled'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging gender pay gap data \n",
    "\n",
    "dfforgdp_copy = dfforgdp.copy()\n",
    "\n",
    "#Rename columns in gender_pay_gap_data to avoid conflicts\n",
    "gender_pay_gap_data = gender_pay_gap_data.rename(columns={'Year': 'Year_gap',  'Controlled': 'Controlled_gap',  'Uncontrolled': 'Uncontrolled_gap'  })\n",
    "\n",
    "#merge the cleaned data with dfforgdp_copy\n",
    "dfforgdp_copy = dfforgdp_copy.merge(gender_pay_gap_data[['Year_gap', 'Uncontrolled_gap', 'Controlled_gap']], how='left', left_on='year', right_on='Year_gap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in labourforce_long to match dfforgdp_copy for easier merging\n",
    "labourforce_long.rename(columns={'Country Code': 'country_region_code', 'Year': 'year', 'Participation Rate': 'labour_force_participation_rate'}, inplace=True)\n",
    "\n",
    "# Merge the dataframes on country_region_code and year, updating dfforgdp_copy\n",
    "dfforgdp_copy = dfforgdp_copy.merge(labourforce_long[['country_region_code', 'year', 'labour_force_participation_rate']], on=['country_region_code', 'year'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter data for the years 2020 to 2022\n",
    "filtered_data = dfforgdp_copy[(dfforgdp_copy['year'] >= 2020) & (dfforgdp_copy['year'] <= 2022)]\n",
    "\n",
    "# Calculate the global average of Controlled and Uncontrolled gender pay gaps for each year\n",
    "global_gender_pay_gap = filtered_data.groupby('year')[['Controlled_gap', 'Uncontrolled_gap']].mean().reset_index()\n",
    "\n",
    "# Plotting the global gender pay gap as a line graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(global_gender_pay_gap['year'], global_gender_pay_gap['Controlled_gap'], marker='o', label='Controlled Gap')\n",
    "plt.plot(global_gender_pay_gap['year'], global_gender_pay_gap['Uncontrolled_gap'], marker='o', label='Uncontrolled Gap')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title(\"Global Gender Pay Gap (2020-2022)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Gender Pay Gap Ratio\")\n",
    "plt.legend(title=\"Type of Pay Gap\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mobility and GDP UNCONTROLLED\n",
    "#Reference: (Brownlee, 2020)\n",
    "\n",
    "# Filter data for years 2020 to 2022 and select only numeric columns\n",
    "df_prophet = dfforgdp_copy[(dfforgdp_copy['year'] >= 2020) & (dfforgdp_copy['year'] <= 2022)].copy()\n",
    "df_prophet = df_prophet.select_dtypes(include=[np.number])  \n",
    "df_prophet['date'] = pd.to_datetime(dfforgdp_copy['date']) \n",
    "\n",
    "# Group by date and calculate mean of numeric columns and renaming columsn\n",
    "df_prophet = df_prophet.groupby('date').mean().reset_index()\n",
    "df_prophet['ds'] = df_prophet['date'] \n",
    "df_prophet['y'] = df_prophet['Uncontrolled_gap']\n",
    "\n",
    "# Initialise the Prophet model\n",
    "model = Prophet()\n",
    "\n",
    "# Add mobility variables and GDP as regressors\n",
    "mobility_predictors = [\n",
    "    'retail_and_recreation_percent_change_from_baseline',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline',\n",
    "    'transit_stations_percent_change_from_baseline',\n",
    "    'workplaces_percent_change_from_baseline',\n",
    "    'residential_percent_change_from_baseline']\n",
    "for predictor in mobility_predictors + ['gdp']:\n",
    "    model.add_regressor(predictor)\n",
    "\n",
    "# Fit the modelusing prophet\n",
    "model.fit(df_prophet[['ds', 'y'] + mobility_predictors + ['gdp']])\n",
    "\n",
    " # Predict for 10 years\n",
    "future = model.make_future_dataframe(periods=10 * 365, freq='D')  # Predict daily for 10 years\n",
    "future = future[future['ds'] >= '2023-01-01']  \n",
    "\n",
    "#Three scenarios for future values\n",
    "\n",
    "#Mobility increases and gdp increases\n",
    "decline_scenario = future.copy()\n",
    "for predictor in mobility_predictors + ['gdp']:\n",
    "    slope_decline = 100 * (df_prophet[predictor].iloc[-1] - df_prophet[predictor].iloc[0]) / len(df_prophet)\n",
    "    decline_scenario[predictor] = df_prophet[predictor].iloc[-1] + slope_decline * np.arange(1, len(future) + 1)\n",
    "\n",
    "#Mobility remains costant and gdp remains constant\n",
    "steady_scenario = future.copy()\n",
    "for predictor in mobility_predictors + ['gdp']:\n",
    "    steady_scenario[predictor] = df_prophet[predictor].iloc[-1]\n",
    "\n",
    "#Mobility declines and GDP declines\n",
    "increase_scenario = future.copy()\n",
    "for predictor in mobility_predictors + ['gdp']:\n",
    "    slope_increase = -100 * (df_prophet[predictor].iloc[-1] - df_prophet[predictor].iloc[0]) / len(df_prophet)\n",
    "    increase_scenario[predictor] = df_prophet[predictor].iloc[-1] + slope_increase * np.arange(1, len(future) + 1)\n",
    "\n",
    "# Forecast for each scenario\n",
    "forecast_increase = model.predict(increase_scenario)\n",
    "forecast_steady = model.predict(steady_scenario)\n",
    "forecast_decline = model.predict(decline_scenario)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df_prophet['ds'], df_prophet['y'], 'o', label='Observed Uncontrolled Gap')\n",
    "plt.plot(forecast_increase['ds'], forecast_increase['yhat'], '--', label='Increase Scenario')\n",
    "plt.plot(forecast_steady['ds'], forecast_steady['yhat'], '--', label='Steady Scenario')\n",
    "plt.plot(forecast_decline['ds'], forecast_decline['yhat'], '--', label='Decline Scenario')\n",
    "\n",
    "\n",
    "plt.title(\"Global Gender Pay Gap Prediction (Uncontrolled) by Mobility and GDP Scenarios (2023-2032)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Uncontrolled Gender Pay Gap Ratio\")\n",
    "plt.legend(title=\"Prediction Scenario\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ba27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mobility and GDP for controlled\n",
    "\n",
    "# Define mobility variables and GDP\n",
    "mobility_predictors = [\n",
    "    'retail_and_recreation_percent_change_from_baseline',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline',\n",
    "    'transit_stations_percent_change_from_baseline',\n",
    "    'workplaces_percent_change_from_baseline',\n",
    "    'residential_percent_change_from_baseline']\n",
    "predictors = mobility_predictors + ['gdp']\n",
    "\n",
    "\n",
    "df_prophet = dfforgdp_copy[(dfforgdp_copy['year'] >= 2020) & (dfforgdp_copy['year'] <= 2022)].copy()\n",
    "df_prophet = df_prophet.select_dtypes(include=[np.number]) \n",
    "df_prophet['date'] = pd.to_datetime(dfforgdp_copy['date'])  \n",
    "\n",
    "df_prophet = df_prophet.groupby('date').mean().reset_index()\n",
    "df_prophet['ds'] = df_prophet['date'] \n",
    "df_prophet['y'] = df_prophet['Controlled_gap']  \n",
    "\n",
    "# Initialise the Prophet model\n",
    "model = Prophet()\n",
    "for predictor in predictors:\n",
    "    model.add_regressor(predictor)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(df_prophet[['ds', 'y'] + predictors])\n",
    "\n",
    "# Define future dates for prediction\n",
    "future = model.make_future_dataframe(periods=10 * 365, freq='D')  \n",
    "\n",
    "# Create scenarios for future values of each mobility variable and GDP\n",
    "increase_scenario = future.copy()\n",
    "steady_scenario = future.copy()\n",
    "decline_scenario = future.copy()\n",
    "\n",
    "for predictor in predictors:\n",
    "    #defining increase and decrease\n",
    "    slope_decline = 4 * (df_prophet[predictor].iloc[-1] - df_prophet[predictor].iloc[0]) / len(df_prophet)\n",
    "    slope_increase = -0.2 * (df_prophet[predictor].iloc[-1] - df_prophet[predictor].iloc[0]) / len(df_prophet)\n",
    "    \n",
    "    # Scenario 1: Increase\n",
    "    decline_scenario[predictor] = df_prophet[predictor].iloc[-1] + slope_decline * np.arange(1, len(future) + 1)\n",
    "    \n",
    "    # Scenario 2: Steady\n",
    "    steady_scenario[predictor] = df_prophet[predictor].iloc[-1]\n",
    "    \n",
    "    # Scenario 3: Decline\n",
    "    increase_scenario[predictor] = df_prophet[predictor].iloc[-1] + slope_increase * np.arange(1, len(future) + 1)\n",
    "\n",
    "# Forecast for each scenario\n",
    "forecast_increase = model.predict(increase_scenario)\n",
    "forecast_steady = model.predict(steady_scenario)\n",
    "forecast_decline = model.predict(decline_scenario)\n",
    "\n",
    "##We want the predictions to be realistic hence the gender pay gap has been capped at 1\n",
    "forecast_increase['yhat'] = np.clip(forecast_increase['yhat'], None, 1)\n",
    "forecast_steady['yhat'] = np.clip(forecast_steady['yhat'], None, 1)\n",
    "forecast_decline['yhat'] = np.clip(forecast_decline['yhat'], None, 1)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df_prophet['ds'], df_prophet['y'], 'o', label='Observed Controlled Gap')\n",
    "plt.plot(forecast_increase['ds'], forecast_increase['yhat'], '--', label='Increase Scenario')\n",
    "plt.plot(forecast_steady['ds'], forecast_steady['yhat'], '--', label='Steady Scenario')\n",
    "plt.plot(forecast_decline['ds'], forecast_decline['yhat'], '--', label='Decline Scenario')\n",
    "\n",
    "\n",
    "plt.title(\"Global Gender Pay Gap Prediction (Controlled) by Mobility and GDP Scenarios (2020-2032)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Controlled Gender Pay Gap Ratio\")\n",
    "plt.legend(title=\"Prediction Scenario\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8193f3",
   "metadata": {},
   "source": [
    "#### Unemployment and mobility patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16614bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unemployment and mobility patterns controlled gender pay gap\n",
    "\n",
    "#mobility variables\n",
    "mobility_predictors = [\n",
    "    'retail_and_recreation_percent_change_from_baseline',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline',\n",
    "    'transit_stations_percent_change_from_baseline',\n",
    "    'workplaces_percent_change_from_baseline',\n",
    "    'residential_percent_change_from_baseline']\n",
    "\n",
    "\n",
    "\n",
    "df_prophet = dfforgdp_copy[(dfforgdp_copy['year'] >= 2020) & (dfforgdp_copy['year'] <= 2022)].copy()\n",
    "df_prophet = df_prophet.select_dtypes(include=[np.number]) \n",
    "df_prophet['date'] = pd.to_datetime(dfforgdp_copy['date']) \n",
    "\n",
    "df_prophet = df_prophet.groupby('date').mean().reset_index()\n",
    "df_prophet['ds'] = df_prophet['date'] \n",
    "# Target variable set to Controlled Gap\n",
    "df_prophet['y'] = df_prophet['Controlled_gap']  \n",
    "\n",
    "# Initialise the Prophet model\n",
    "model = Prophet()\n",
    "\n",
    "# Add unemployment and each mobility variable as regressors\n",
    "model.add_regressor('unemployment_rate_observed')\n",
    "for predictor in mobility_predictors:\n",
    "    model.add_regressor(predictor)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(df_prophet[['ds', 'y', 'unemployment_rate_observed'] + mobility_predictors])\n",
    "\n",
    "\n",
    "future = model.make_future_dataframe(periods=10 * 365, freq='D')  \n",
    "future = future[future['ds'] >= '2023-01-01']  \n",
    "\n",
    "\n",
    "increase_scenario = future.copy()\n",
    "steady_scenario = future.copy()\n",
    "decline_scenario = future.copy()\n",
    "\n",
    "\n",
    "for predictor in ['unemployment_rate_observed'] + mobility_predictors:\n",
    "    # Define increase and decline scenarios\n",
    "    slope_increase = 0.5 * (df_prophet[predictor].iloc[-1] - df_prophet[predictor].iloc[0]) / len(df_prophet)\n",
    "    slope_decline = -1 * (df_prophet[predictor].iloc[-1] - df_prophet[predictor].iloc[0]) / len(df_prophet)\n",
    "    \n",
    "    # Scenario 1: Increase\n",
    "    increase_scenario[predictor] = df_prophet[predictor].iloc[-1] + slope_increase * np.arange(1, len(future) + 1)\n",
    "    \n",
    "    # Scenario 2: Steady\n",
    "    steady_scenario[predictor] = df_prophet[predictor].iloc[-1]\n",
    "    \n",
    "    # Scenario 3: Decline\n",
    "    decline_scenario[predictor] = df_prophet[predictor].iloc[-1] + slope_decline * np.arange(1, len(future) + 1)\n",
    "\n",
    "# Forecast for each scenario\n",
    "forecast_increase = model.predict(increase_scenario)\n",
    "forecast_steady = model.predict(steady_scenario)\n",
    "forecast_decline = model.predict(decline_scenario)\n",
    "\n",
    "# Cap predictions at 1\n",
    "forecast_increase['yhat'] = np.clip(forecast_increase['yhat'], None, 1)\n",
    "forecast_steady['yhat'] = np.clip(forecast_steady['yhat'], None, 1)\n",
    "forecast_decline['yhat'] = np.clip(forecast_decline['yhat'], None, 1)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df_prophet['ds'], df_prophet['y'], 'o', label='Observed Controlled Gap')\n",
    "plt.plot(forecast_increase['ds'], forecast_increase['yhat'], '--', label='Increase Scenario')\n",
    "plt.plot(forecast_steady['ds'], forecast_steady['yhat'], '--', label='Steady Scenario')\n",
    "plt.plot(forecast_decline['ds'], forecast_decline['yhat'], '--', label='Decline Scenario')\n",
    "\n",
    "\n",
    "plt.title(\"Global Gender Pay Gap Prediction (Controlled) by Unemployment and Mobility Scenarios (2023-2032)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Controlled Gender Pay Gap Ratio\")\n",
    "plt.legend(title=\"Prediction Scenario\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa388d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mobility and unemployment for uncontrolled gender pay gap\n",
    "\n",
    "# Define mobility variables\n",
    "mobility_predictors = [\n",
    "    'retail_and_recreation_percent_change_from_baseline',\n",
    "    'grocery_and_pharmacy_percent_change_from_baseline',\n",
    "    'parks_percent_change_from_baseline',\n",
    "    'transit_stations_percent_change_from_baseline',\n",
    "    'workplaces_percent_change_from_baseline',\n",
    "    'residential_percent_change_from_baseline']\n",
    "\n",
    "df_prophet = dfforgdp_copy[(dfforgdp_copy['year'] >= 2020) & (dfforgdp_copy['year'] <= 2022)].copy()\n",
    "df_prophet = df_prophet.select_dtypes(include=[np.number])  \n",
    "df_prophet['date'] = pd.to_datetime(dfforgdp_copy['date'])  \n",
    "\n",
    "\n",
    "df_prophet = df_prophet.groupby('date').mean().reset_index()\n",
    "df_prophet['ds'] = df_prophet['date'] \n",
    "df_prophet['y'] = df_prophet['Uncontrolled_gap']\n",
    "\n",
    "# Initialise the Prophet model\n",
    "model = Prophet()\n",
    "\n",
    "model.add_regressor('unemployment_rate_observed')\n",
    "for predictor in mobility_predictors:\n",
    "    model.add_regressor(predictor)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(df_prophet[['ds', 'y', 'unemployment_rate_observed'] + mobility_predictors])\n",
    "\n",
    "\n",
    "future = model.make_future_dataframe(periods=10 * 365, freq='D')  \n",
    "future = future[future['ds'] >= '2023-01-01']  \n",
    "\n",
    "\n",
    "increase_scenario = future.copy()\n",
    "steady_scenario = future.copy()\n",
    "decline_scenario = future.copy()\n",
    "\n",
    "for predictor in ['unemployment_rate_observed'] + mobility_predictors:\n",
    "    # Define increase and decline slopes\n",
    "    slope_increase = 100 * (df_prophet[predictor].iloc[-1] - df_prophet[predictor].iloc[0]) / len(df_prophet)\n",
    "    slope_decline = -100 * (df_prophet[predictor].iloc[-1] - df_prophet[predictor].iloc[0]) / len(df_prophet)\n",
    "    \n",
    "    # Scenario 1: Increase\n",
    "    increase_scenario[predictor] = df_prophet[predictor].iloc[-1] + slope_increase * np.arange(1, len(future) + 1)\n",
    "    \n",
    "    # Scenario 2: Steady\n",
    "    steady_scenario[predictor] = df_prophet[predictor].iloc[-1]\n",
    "    \n",
    "    # Scenario 3: Decline\n",
    "    decline_scenario[predictor] = df_prophet[predictor].iloc[-1] + slope_decline * np.arange(1, len(future) + 1)\n",
    "\n",
    "# Forecast for each scenario\n",
    "forecast_increase = model.predict(increase_scenario)\n",
    "forecast_steady = model.predict(steady_scenario)\n",
    "forecast_decline = model.predict(decline_scenario)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df_prophet['ds'], df_prophet['y'], 'o', label='Observed Uncontrolled Gap')\n",
    "plt.plot(forecast_increase['ds'], forecast_increase['yhat'], '--', label='Increase Scenario')\n",
    "plt.plot(forecast_steady['ds'], forecast_steady['yhat'], '--', label='Steady Scenario')\n",
    "plt.plot(forecast_decline['ds'], forecast_decline['yhat'], '--', label='Decline Scenario')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(\"Global Gender Pay Gap Prediction (Uncontrolled) by Unemployment and Mobility Scenarios (2023-2032)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Uncontrolled Gender Pay Gap Ratio\")\n",
    "plt.legend(title=\"Prediction Scenario\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ae74c",
   "metadata": {},
   "source": [
    "### References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57447027",
   "metadata": {},
   "source": [
    "alenavorushilova (2020). Missing Data and NAs Guide. [online] Kaggle.com. Available at: https://www.kaggle.com/code/alenavorushilova/missing-data-and-nas-guide [Accessed 22 Oct. 2024].\n",
    "\n",
    "Brownlee, J. (2020). Time Series Forecasting With Prophet in Python - MachineLearningMastery.com. [online] MachineLearningMastery.com. Available at: https://machinelearningmastery.com/time-series-forecasting-with-prophet-in-python/ [Accessed 3 Nov. 2024].\n",
    "\n",
    "Gabriel (2016). defining max and min yaxis values after using ax.set_yscale(‘log’) in matplotlib python. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/39391589/defining-max-and-min-yaxis-values-after-using-ax-set-yscalelog-in-matplotlib [Accessed 27 Oct. 2024].\n",
    "\n",
    "GeeksforGeeks (2018). Principal Component Analysis with Python. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/principal-component-analysis-with-python/ [Accessed 25 Oct. 2024].\n",
    "\n",
    "GeeksforGeeks (2019). Elbow Method for optimal value of k in KMeans. [online] GeeksforGeeks. Available at: https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/ [Accessed 24 Oct. 2024].\n",
    "\n",
    "Google (2022). COVID-19 Community Mobility Reports. [online] COVID-19 Community Mobility Report. Available at: https://www.google.com/covid19/mobility/ [Accessed 22 Oct. 2024].\n",
    "\n",
    "Gorn, K. (2021). Polar plot xtick label position. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/66007742/polar-plot-xtick-label-position [Accessed 27 Oct. 2024].\n",
    "\n",
    "Holtz, Y. (2019). Dual Y axis with Python and Matplotlib. [online] The Python Graph Gallery. Available at: https://python-graph-gallery.com/line-chart-dual-y-axis-with-matplotlib/ [Accessed 28 Oct. 2024].\n",
    "\n",
    "JPV (2017). MonthLocator in Matplotlib. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/42881610/monthlocator-in-matplotlib [Accessed 23 Oct. 2024].\n",
    "\n",
    "Kumar, A. (2020). KMeans Silhouette Score Explained With Python Example. [online] dzone.com. Available at: https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam [Accessed 24 Oct. 2024].\n",
    "\n",
    "Mathieu, E., Ritchie, H., Rodés-Guirao, L., Appel, C., Giattino, C., Hasell, J., Macdonald, B., Saloni Dattani, Beltekian, D., Ortiz-Ospina, E. and Roser, M. (2020). Coronavirus Pandemic (COVID-19). [online] Our World in Data. Available at: https://ourworldindata.org/covid-stringency-index [Accessed 1 Nov. 2024].\n",
    "\n",
    "my (2020). Trying to make my labels legible in scipy dendrogram. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/64978179/trying-to-make-my-labels-legible-in-scipy-dendrogram [Accessed 1 Nov. 2024].\n",
    "\n",
    "Mycompiler.io. (2024). Radar Chart (Python) - myCompiler. [online] Available at: https://www.mycompiler.io/view/3vsrByqBiF5 [Accessed 27 Oct. 2024].\n",
    "\n",
    "Naturaldisasters.ai. (2023). How to Plot a World Map Using Python and GeoPandas | NaturalDisasters.ai. [online] Available at: https://naturaldisasters.ai/posts/python-geopandas-world-map-tutorial/ [Accessed 24 Oct. 2024].\n",
    "\n",
    "Our World in Data. (2017). Global GDP over the long run. [online] Available at: https://ourworldindata.org/grapher/global-gdp-over-the-long-run?tab=table [Accessed 2 Nov. 2024].\n",
    "\n",
    "Our World in Data. (2020). Annual GDP growth. [online] Available at: https://ourworldindata.org/grapher/real-gdp-growth [Accessed 2 Nov. 2024].\n",
    "\n",
    "Our World in Data. (2022). COVID-19 Data Explorer. [online] Available at: https://ourworldindata.org/explorers/covid [Accessed 27 Oct. 2024].\n",
    "\n",
    "Our World in Data. (2023). Unemployment rate. [online] Available at: https://ourworldindata.org/grapher/unemployment-rate-imf?tab=table&time=1982..latest [Accessed 2 Nov. 2024].\n",
    "\n",
    "Plotly (2020). Annotations on plotly Choropleth. [online] Plotly Community Forum. Available at: https://community.plotly.com/t/annotations-on-plotly-choropleth/36219 [Accessed 24 Oct. 2024].\n",
    "\n",
    "ryilkici (2020). How to enlarge geographic map in Python/Plotly choropleth plot? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/63466163/how-to-enlarge-geographic-map-in-python-plotly-choropleth-plot [Accessed 24 Oct. 2024].\n",
    "\n",
    "scikit-learn. (2024). LinearRegression. [online] Available at: https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html [Accessed 1 Nov. 2024].\n",
    "\n",
    "Statista. (2024a). Gender gap index 2024 | Statista. [online] Available at: https://www.statista.com/statistics/244387/the-global-gender-gap-index/ [Accessed 3 Nov. 2024].\n",
    "\n",
    "Statista. (2024b). Gender pay gap worldwide 2024 | Statista. [online] Available at: https://www.statista.com/statistics/1212140/global-gender-pay-gap/#:~:text=By%20comparison%2C%20the%20uncontrolled%20gender,every%20dollar%20earned%20by%20men. [Accessed 2 Nov. 2024].\n",
    "\n",
    "Statista. (2024c). Workplace gender gap worldwide by type 2024 | Statista. [online] Available at: https://www.statista.com/statistics/1212189/workplace-gender-gap-worldwide-by-type/ [Accessed 3 Nov. 2024].\n",
    "\n",
    "Tavares, E. (2017). Variance Inflation Factor (VIF) Explained - Python. [online] Github.io. Available at: https://etav.github.io/python/vif_factor_python.html [Accessed 23 Oct. 2024].\n",
    "\n",
    "The (2024). Transport – The Covid-19 Crisis and Clean Energy Progress – Analysis - IEA. [online] IEA. Available at: https://www.iea.org/reports/the-covid-19-crisis-and-clean-energy-progress/transport [Accessed 1 Nov. 2024].\n",
    "\n",
    "World Bank Open Data. (2024). World Bank Open Data. [online] Available at: https://data.worldbank.org/indicator/SL.TLF.CACT.FE.ZS [Accessed 2 Nov. 2024]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7f9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
